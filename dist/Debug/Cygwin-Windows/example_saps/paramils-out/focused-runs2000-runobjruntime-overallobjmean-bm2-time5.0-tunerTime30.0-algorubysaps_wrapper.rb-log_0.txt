Call: /usr/bin/ruby ../scripts/param_ils_2_2_run.rb "-numRun" "0" "-scenariofile" "example_saps/scenario-Saps-SWGCP-sat-small-train-small-test.txt" "-validN" "100"


seed: 1234
algo: ruby saps_wrapper.rb
tunerTimeout: 30.0
maxEvals: 100000
run_obj: runtime
overall_obj: mean
instance_file: example_data/SWGCP-satisfiable-small-train.txt
test_instance_file: example_data/SWGCP-satisfiable-small-test.txt
N: 2000
cutoff_time: 5.0
cutoff_length: 2147483647
R: 10
pertubation_strength_basic: 
pertubation_strength_scaling: false
p_restart: 0.01
Run 1
Level 
========================================================
Starting ILS for level 1, i.e. a limit of N=2000, and cutoff time=5.0.
Current CPU time = 0, this run goes until 30.0 
========================================================
New Incumbent: 0, 100000000 [0, 0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
 Same incumbent, new precision:
New Incumbent: 0.1, 0.0 [1, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
          -> Take improving step to random alpha=1.189 ps=0.2 rho=0.83 wp=0.04 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.256 ps=0 rho=0.17 wp=0.01 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.066 ps=0.066 rho=1 wp=0.03 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.4 ps=0.133 rho=0.666 wp=0.02 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.4 ps=0.066 rho=0 wp=0 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.256 ps=0.166 rho=0 wp=0.01 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.4 ps=0.2 rho=0.333 wp=0 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.189 ps=0.133 rho=0.83 wp=0.02 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.4 ps=0.066 rho=0.5 wp=0.03 (0.0 [based on 1 runs with cutoff 5.0])

          -> Take improving step to random alpha=1.01 ps=0.033 rho=0.5 wp=0 (0.0 [based on 1 runs with cutoff 5.0])

   BLS in iteration 1, start with alpha=1.01 ps=0.033 rho=0.5 wp=0 (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.01->1.256, evaluating ...
          -> Take improving step to neighbour alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 1 runs with cutoff 5.0]) with flip 1

          
============= Performing 1 bonus runs of state: alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 1 runs with cutoff 5.0]) ============ 

State wants more detail (1+1) than incumbent (1), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 1 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 1 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 1.3, 0.0 [2, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
          -> After 1 bonus runs: alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 2 runs with cutoff 5.0])

    Changing alpha: 1.256->1.126, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.033->0.1, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.5->0.17, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.066, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.5->0.333, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.5->0.666, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.033->0.2, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.033->0.066, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.4, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0->0.05, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.033->0.133, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.5->1, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.033->0.166, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.189, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.5->0, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0->0.03, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0->0.06, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.033->0, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0->0.01, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.5->0.83, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0->0.02, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0->0.04, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.326, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
          
============= Performing 23 bonus runs of state: alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 2 runs with cutoff 5.0]) ============ 

State wants more detail (2+1) than incumbent (2), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 2 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 2 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 3.8, 0.0 [3, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (3+1) than incumbent (3), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 3 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 3 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 4.0, 0.0 [4, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (4+1) than incumbent (4), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 4 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 4 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 4.2, 0.0 [5, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (5+1) than incumbent (5), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 5 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 5 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 4.4, 0.0 [6, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (6+1) than incumbent (6), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 6 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 6 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 4.6, 0.0 [7, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (7+1) than incumbent (7), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 7 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 7 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 4.8, 0.0 [8, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (8+1) than incumbent (8), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 8 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 8 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 5.0, 0.0 [9, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (9+1) than incumbent (9), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 9 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 9 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 5.2, 0.0 [10, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (10+1) than incumbent (10), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 10 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 10 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 5.4, 0.0 [11, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (11+1) than incumbent (11), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 11 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 11 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 5.6, 0.0 [12, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (12+1) than incumbent (12), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 12 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 12 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 5.8, 0.0 [13, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (13+1) than incumbent (13), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 13 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 13 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 5.99999999999999, 0.0 [14, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (14+1) than incumbent (14), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 14 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 14 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 6.19999999999999, 0.0 [15, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (15+1) than incumbent (15), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 15 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 15 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 6.39999999999999, 0.0 [16, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (16+1) than incumbent (16), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 16 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 16 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 6.59999999999999, 0.0 [17, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (17+1) than incumbent (17), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 17 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 17 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 6.79999999999999, 0.0 [18, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (18+1) than incumbent (18), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 18 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 18 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 6.99999999999999, 0.0 [19, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (19+1) than incumbent (19), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 19 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 19 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 7.19999999999999, 0.0 [20, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (20+1) than incumbent (20), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 20 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 20 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 7.39999999999999, 0.0 [21, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (21+1) than incumbent (21), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 21 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 21 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 7.59999999999999, 0.0 [22, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (22+1) than incumbent (22), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 22 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 22 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 7.79999999999999, 0.0 [23, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (23+1) than incumbent (23), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 23 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 23 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 7.99999999999999, 0.0 [24, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
State wants more detail (24+1) than incumbent (24), doing incumbent first:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 24 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 24 runs with cutoff 5.0])
 Same incumbent, new precision:
New Incumbent: 8.19999999999999, 0.0 [25, 5.0]. With state alpha=1.189, ps=0.1, rho=0.5, wp=0.03
          -> After 23 bonus runs for LM: alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 25 runs with cutoff 5.0])

   LM for iteration 1: alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 25 runs with cutoff 5.0])

========== DETAILED RESULTS (iteration 1): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 1): alpha=1.189, wp=0.03, rho=0.5, ps=0.1
==================================================================
Training quality of this incumbent parameter configuration: 0.0, based on 25 runs with cutoff 5.0
==================================================================

Comparing LM against incumbent:
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 25 runs with cutoff 5.0])
alpha=1.189 ps=0.1 rho=0.5 wp=0.03 (0.0 [based on 25 runs with cutoff 5.0])
LM better, change incumbent
New Incumbent: 8.29999999999999, 0.0 [25, 5.0]. With state alpha=1.256, ps=0.033, rho=0.5, wp=0
83/100000, 8.29999999999999/30.0
iteration 2, flip 3, evaluation count 83
    perturb to ---> alpha=1.256 ps=0.1 rho=0.5 wp=0 (0.0 [based on 1 runs with cutoff 5.0])
    perturb to ---> alpha=1.256 ps=0.1 rho=0.17 wp=0 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> alpha=1.256 ps=0.1 rho=0.83 wp=0 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 2, start with alpha=1.256 ps=0.1 rho=0.83 wp=0 (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0->0.04, evaluating ...
          -> Take improving step to neighbour alpha=1.256 ps=0.1 rho=0.83 wp=0.04 (0.0 [based on 1 runs with cutoff 5.0]) with flip 3

          
============= Performing 1 bonus runs of state: alpha=1.256 ps=0.1 rho=0.83 wp=0.04 (0.0 [based on 1 runs with cutoff 5.0]) ============ 

          -> After 1 bonus runs: alpha=1.256 ps=0.1 rho=0.83 wp=0.04 (0.0 [based on 2 runs with cutoff 5.0])

    Changing wp: 0.04->0.02, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.83->0.17, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.83->0.5, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.1->0.2, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.1->0.033, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.1->0.166, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.1->0, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.126, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0.04->0.01, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.1->0.133, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0.04->0.05, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.83->0, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.4, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.326, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.83->0.333, evaluating ...
101/100000, 10.1/30.0
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.83->1, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.1->0.066, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0.04->0.06, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.189, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0.04->0.03, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.01, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing alpha: 1.256->1.066, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.83->0.666, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
          
============= Performing 23 bonus runs of state: alpha=1.256 ps=0.1 rho=0.83 wp=0.04 (0.0 [based on 2 runs with cutoff 5.0]) ============ 

          -> After 23 bonus runs for LM: alpha=1.256 ps=0.1 rho=0.83 wp=0.04 (0.0 [based on 25 runs with cutoff 5.0])

   LM for iteration 2: alpha=1.256 ps=0.1 rho=0.83 wp=0.04 (0.0 [based on 25 runs with cutoff 5.0])

========== DETAILED RESULTS (iteration 2): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 2): alpha=1.256, wp=0, rho=0.5, ps=0.033
==================================================================
Training quality of this incumbent parameter configuration: 0.0, based on 25 runs with cutoff 5.0
==================================================================

Comparing LM against incumbent:
alpha=1.256 ps=0.1 rho=0.83 wp=0.04 (0.0 [based on 25 runs with cutoff 5.0])
alpha=1.256 ps=0.033 rho=0.5 wp=0 (0.0 [based on 25 runs with cutoff 5.0])
LM better, change incumbent
New Incumbent: 13.2, 0.0 [25, 5.0]. With state alpha=1.256, ps=0.1, rho=0.83, wp=0.04
   Accepting new better local optimum: alpha=1.256 ps=0.1 rho=0.83 wp=0.04 (0.0 [based on 25 runs with cutoff 5.0])
132/100000, 13.2/30.0
iteration 3, flip 5, evaluation count 132
    perturb to ---> alpha=1.256 ps=0.1 rho=0.83 wp=0.01 (0.0 [based on 1 runs with cutoff 5.0])
    perturb to ---> alpha=1.256 ps=0.2 rho=0.83 wp=0.01 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> alpha=1.256 ps=0.2 rho=0.83 wp=0.06 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 3, start with alpha=1.256 ps=0.2 rho=0.83 wp=0.06 (0.0 [based on 1 runs with cutoff 5.0])
    Changing rho: 0.83->0.333, evaluating ...
          -> Take improving step to neighbour alpha=1.256 ps=0.2 rho=0.333 wp=0.06 (0.0 [based on 1 runs with cutoff 5.0]) with flip 5

          
============= Performing 1 bonus runs of state: alpha=1.256 ps=0.2 rho=0.333 wp=0.06 (0.0 [based on 1 runs with cutoff 5.0]) ============ 

          -> After 1 bonus runs: alpha=1.256 ps=0.2 rho=0.333 wp=0.06 (0.0 [based on 2 runs with cutoff 5.0])

    Changing ps: 0.2->0.166, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.2->0, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing ps: 0.2->0.1, evaluating ...
        -> worse: (0.0 [based on 1 runs with cutoff 5.0])
    Changing wp: 0.06->0.03, evaluating ...
