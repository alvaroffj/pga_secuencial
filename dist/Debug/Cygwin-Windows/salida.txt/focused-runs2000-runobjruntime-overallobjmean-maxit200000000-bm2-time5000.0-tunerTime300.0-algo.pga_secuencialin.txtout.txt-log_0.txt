Call: /usr/bin/ruby ../scripts/param_ils_2_3_run.rb "-numRun" "0" "-scenariofile" "escenario.txt" "-validN" "5"


seed: 1234
algo: ./pga_secuencial in.txt out.txt
tunerTimeout: 300.0
maxEvals: 100000000
run_obj: runtime
overall_obj: mean
instance_file: in.txt
test_instance_file: in.txt
N: 2000
cutoff_time: 5000.0
cutoff_length: 200000
R: 10
pertubation_strength_basic: 
pertubation_strength_scaling: false
p_restart: 0.01
Run 1
Level 
========================================================
Starting ILS for level 1, i.e. a limit of N=2000, and cutoff time=5000.0.
Current CPU time = 0, this run goes until 300.0 
========================================================
New Incumbent: 0, 100000000 [0, 0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 0.1, -1.0e-05 [1, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> Take improving step to random c=0.6 g=80 m=0.3 po=800 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.9 g=100 m=0.1 po=2400 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.6 g=40 m=0.2 po=2800 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.7 g=80 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.9 g=60 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.9 g=20 m=0.1 po=800 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.9 g=40 m=0.3 po=100 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.9 g=80 m=0.1 po=800 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.7 g=60 m=0.1 po=2400 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

          -> Take improving step to random c=0.6 g=20 m=0.3 po=200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])

   BLS in iteration 1, start with c=0.6 g=20 m=0.3 po=200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) with flip 1

          
============= Performing 1 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) ============ 

State wants more detail (1+1) than incumbent (1), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 1.3, -1.0e-05 [2, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 1 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])

    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->400, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
          
============= Performing 22 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) ============ 

State wants more detail (2+1) than incumbent (2), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 3.7, -1.0e-05 [3, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (3+1) than incumbent (3), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 3.9, -1.0e-05 [4, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (4+1) than incumbent (4), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 4 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 4 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 4.1, -1.0e-05 [5, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (5+1) than incumbent (5), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 4.3, -1.0e-05 [6, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (6+1) than incumbent (6), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 6 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 6 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 4.5, -1.0e-05 [7, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (7+1) than incumbent (7), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 7 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 7 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 4.7, -1.0e-05 [8, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (8+1) than incumbent (8), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 8 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 8 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 4.9, -1.0e-05 [9, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (9+1) than incumbent (9), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 5.1, -1.0e-05 [10, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (10+1) than incumbent (10), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 10 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 10 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 5.3, -1.0e-05 [11, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (11+1) than incumbent (11), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 11 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 11 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 5.5, -1.0e-05 [12, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (12+1) than incumbent (12), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 12 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 12 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 5.7, -1.0e-05 [13, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (13+1) than incumbent (13), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 13 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 13 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 5.9, -1.0e-05 [14, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (14+1) than incumbent (14), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 14 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 14 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 6.09999999999999, -1.0e-05 [15, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (15+1) than incumbent (15), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 15 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 15 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 6.29999999999999, -1.0e-05 [16, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (16+1) than incumbent (16), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 16 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 16 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 6.49999999999999, -1.0e-05 [17, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (17+1) than incumbent (17), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 17 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 17 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 6.69999999999999, -1.0e-05 [18, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (18+1) than incumbent (18), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 18 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 18 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 6.89999999999999, -1.0e-05 [19, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (19+1) than incumbent (19), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 19 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 19 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 7.09999999999999, -1.0e-05 [20, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (20+1) than incumbent (20), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 20 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 20 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 7.29999999999999, -1.0e-05 [21, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (21+1) than incumbent (21), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 21 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 21 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 7.49999999999999, -1.0e-05 [22, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (22+1) than incumbent (22), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 22 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 22 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 7.69999999999999, -1.0e-05 [23, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (23+1) than incumbent (23), doing incumbent first:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 23 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 23 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 7.89999999999999, -1.0e-05 [24, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 22 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 24 runs with cutoff 5000.0])

   LM for iteration 1: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 24 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 1): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 1): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 24 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 24 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (-1.0e-05 [based on 24 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 7.99999999999999, -1.0e-05 [24, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
80/100000000, 7.99999999999999/300.0
iteration 2, flip 3, evaluation count 80
    perturb to ---> c=0.6 g=20 m=0.3 po=3200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=1600 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
   BLS in iteration 2, start with c=0.6 g=20 m=0.3 po=1600 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 1600->400, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) with flip 3

          
============= Performing 3 bonus runs of state: c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 3 bonus runs: c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0])

    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->50, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 24 runs with cutoff 5000.0]) with flip 4

          
============= Performing 13 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 24 runs with cutoff 5000.0]) ============ 

101/100000000, 10.1/300.0
 Same incumbent, new precision:
New Incumbent: 10.1, -1.0e-05 [25, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 10.2, -1.0e-05 [26, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 10.3, -1.0e-05 [27, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 10.4, -1.0e-05 [28, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 10.5, -1.0e-05 [29, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 10.6, -1.0e-05 [30, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 10.7, -1.0e-05 [31, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 10.8, -1.0e-05 [32, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 10.9, -1.0e-05 [33, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 11.0, -1.0e-05 [34, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 11.1, -1.0e-05 [35, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 11.2, -1.0e-05 [36, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 11.3, -1.0e-05 [37, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 13 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 37 runs with cutoff 5000.0])

    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
          
============= Performing 15 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 37 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 12.9, -1.0e-05 [38, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.0, -1.0e-05 [39, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.1, -1.0e-05 [40, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.2, -1.0e-05 [41, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.3, -1.0e-05 [42, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.4, -1.0e-05 [43, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.5, -1.0e-05 [44, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.6, -1.0e-05 [45, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.7, -1.0e-05 [46, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.8, -1.0e-05 [47, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 13.9, -1.0e-05 [48, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 14.0, -1.0e-05 [49, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 14.1, -1.0e-05 [50, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 14.2, -1.0e-05 [51, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 14.3, -1.0e-05 [52, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 15 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 52 runs with cutoff 5000.0])

   LM for iteration 2: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 52 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 2): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 2): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 52 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 52 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 52 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 14.3, -1.0e-05 [52, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 52 runs with cutoff 5000.0])
143/100000000, 14.3/300.0
iteration 3, flip 6, evaluation count 143
    perturb to ---> c=0.6 g=20 m=0.2 po=100 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.2 po=2400 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.6 g=20 m=0.2 po=1600 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 3, start with c=0.6 g=20 m=0.2 po=1600 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 1600->3600, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.2 po=3600 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) with flip 6

          
============= Performing 1 bonus runs of state: c=0.6 g=20 m=0.2 po=3600 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=20 m=0.2 po=3600 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])

    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.2->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->400, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->50, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.2->0.3, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0]) with flip 7

          
============= Performing 16 bonus runs of state: c=0.6 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 16 bonus runs: c=0.6 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 19 runs with cutoff 5000.0])

    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->50, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 3600->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 3600->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 3600->200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 3600->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 3600->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 52 runs with cutoff 5000.0]) with flip 8

          
============= Performing 8 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 52 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 18.7, -1.0e-05 [53, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 18.8, -1.0e-05 [54, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 18.9, -1.0e-05 [55, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 19.0, -1.0e-05 [56, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 19.1, -1.0e-05 [57, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 19.2, -1.0e-05 [58, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 19.3, -1.0e-05 [59, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 19.4, -1.0e-05 [60, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 8 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 60 runs with cutoff 5000.0])

    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
201/100000000, 20.1/300.0
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 100->400, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
          
============= Performing 17 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 60 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 21.2, -1.0e-05 [61, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 21.3, -1.0e-05 [62, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 21.4, -1.0e-05 [63, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 21.5, -1.0e-05 [64, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 21.6, -1.0e-05 [65, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 21.7, -1.0e-05 [66, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 21.8, -1.0e-05 [67, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 21.9, -1.0e-05 [68, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.0, -1.0e-05 [69, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.1, -1.0e-05 [70, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.2, -1.0e-05 [71, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.3, -1.0e-05 [72, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.4, -1.0e-05 [73, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.5, -1.0e-05 [74, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.6000000000001, -1.0e-05 [75, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.7000000000001, -1.0e-05 [76, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 22.8000000000001, -1.0e-05 [77, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 17 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 77 runs with cutoff 5000.0])

   LM for iteration 3: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 77 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 3): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 3): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 77 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 77 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 77 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 22.8000000000001, -1.0e-05 [77, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 77 runs with cutoff 5000.0])
228/100000000, 22.8000000000001/300.0
iteration 4, flip 10, evaluation count 228
    perturb to ---> c=0.6 g=20 m=0.3 po=1600 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=100 m=0.3 po=1600 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=100 m=0.3 po=200 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 4, start with c=0.6 g=100 m=0.3 po=200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->50, evaluating ...
          -> Take improving step to neighbour c=0.6 g=100 m=0.3 po=50 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) with flip 10

          
============= Performing 1 bonus runs of state: c=0.6 g=100 m=0.3 po=50 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=100 m=0.3 po=50 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])

    Changing po: 50->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=100 m=0.3 po=100 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0]) with flip 11

          
============= Performing 7 bonus runs of state: c=0.6 g=100 m=0.3 po=100 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 7 bonus runs: c=0.6 g=100 m=0.3 po=100 pr=1 (-1.0e-05 [based on 10 runs with cutoff 5000.0])

    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 100->20, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 77 runs with cutoff 5000.0]) with flip 12

          
============= Performing 2 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 77 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 24.8000000000001, -1.0e-05 [78, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 24.9000000000001, -1.0e-05 [79, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 2 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 79 runs with cutoff 5000.0])

    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->400, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 21 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
          
============= Performing 22 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 79 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 27.2000000000001, -1.0e-05 [80, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 27.3000000000001, -1.0e-05 [81, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 27.4000000000001, -1.0e-05 [82, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 27.5000000000001, -1.0e-05 [83, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 27.6000000000001, -1.0e-05 [84, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 27.7000000000001, -1.0e-05 [85, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 27.8000000000001, -1.0e-05 [86, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 27.9000000000001, -1.0e-05 [87, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.0000000000001, -1.0e-05 [88, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.1000000000001, -1.0e-05 [89, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.2000000000001, -1.0e-05 [90, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.3000000000001, -1.0e-05 [91, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.4000000000001, -1.0e-05 [92, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.5000000000001, -1.0e-05 [93, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.6000000000001, -1.0e-05 [94, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.7000000000001, -1.0e-05 [95, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.8000000000001, -1.0e-05 [96, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 28.9000000000001, -1.0e-05 [97, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 29.0000000000001, -1.0e-05 [98, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 29.1000000000001, -1.0e-05 [99, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 29.2000000000001, -1.0e-05 [100, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 29.3000000000001, -1.0e-05 [101, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 22 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 101 runs with cutoff 5000.0])

   LM for iteration 4: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 101 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 4): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 4): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 101 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 101 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 101 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 29.3000000000001, -1.0e-05 [101, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 101 runs with cutoff 5000.0])
293/100000000, 29.3000000000001/300.0
iteration 5, flip 14, evaluation count 293
    perturb to ---> c=0.6 g=20 m=0.3 po=2000 pr=1 (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=80 m=0.3 po=2000 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.6 g=80 m=0.3 po=2400 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 5, start with c=0.6 g=80 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->20, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 4 runs with cutoff 5000.0]) with flip 14

          
============= Performing 1 bonus runs of state: c=0.6 g=20 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 4 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=20 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0])

    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->2800, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0]) with flip 15

          
============= Performing 2 bonus runs of state: c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0]) ============ 

          -> After 2 bonus runs: c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 7 runs with cutoff 5000.0])

    Changing g: 20->60, evaluating ...
301/100000000, 30.1000000000002/300.0
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2800->50, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 2800->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 2800->400, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 8 runs with cutoff 5000.0]) with flip 16

          
============= Performing 5 bonus runs of state: c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 8 runs with cutoff 5000.0]) ============ 

          -> After 5 bonus runs: c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 13 runs with cutoff 5000.0])

    Changing po: 400->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 101 runs with cutoff 5000.0]) with flip 17

          
============= Performing 1 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 101 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 31.2000000000002, -1.0e-05 [102, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 1 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 102 runs with cutoff 5000.0])

    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 22 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
          
============= Performing 18 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 102 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 33.1000000000002, -1.0e-05 [103, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 33.2000000000002, -1.0e-05 [104, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 33.3000000000002, -1.0e-05 [105, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 33.4000000000002, -1.0e-05 [106, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 33.5000000000002, -1.0e-05 [107, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 33.6000000000002, -1.0e-05 [108, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 33.7000000000002, -1.0e-05 [109, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 33.8000000000002, -1.0e-05 [110, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 33.9000000000002, -1.0e-05 [111, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.0000000000002, -1.0e-05 [112, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.1000000000002, -1.0e-05 [113, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.2000000000002, -1.0e-05 [114, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.3000000000002, -1.0e-05 [115, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.4000000000002, -1.0e-05 [116, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.5000000000002, -1.0e-05 [117, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.6000000000002, -1.0e-05 [118, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.7000000000002, -1.0e-05 [119, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 34.8000000000002, -1.0e-05 [120, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 18 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 120 runs with cutoff 5000.0])

   LM for iteration 5: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 120 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 5): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 5): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 120 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 120 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 120 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 34.8000000000002, -1.0e-05 [120, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 120 runs with cutoff 5000.0])
348/100000000, 34.8000000000002/300.0
iteration 6, flip 19, evaluation count 348
    perturb to ---> c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=1200 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 22 runs with cutoff 5000.0])
   BLS in iteration 6, start with c=0.6 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 23 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing po: 3600->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing po: 3600->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing po: 3600->50, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 3600->30, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->400, evaluating ...
        -> worse: (-1.0e-05 [based on 15 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 120 runs with cutoff 5000.0]) with flip 19

          
============= Performing 12 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 120 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 36.2000000000002, -1.0e-05 [121, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 36.3000000000002, -1.0e-05 [122, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 36.4000000000002, -1.0e-05 [123, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 36.5000000000002, -1.0e-05 [124, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 36.6000000000003, -1.0e-05 [125, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 36.7000000000003, -1.0e-05 [126, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 36.8000000000003, -1.0e-05 [127, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 36.9000000000003, -1.0e-05 [128, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 37.0000000000003, -1.0e-05 [129, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 37.1000000000003, -1.0e-05 [130, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 37.2000000000003, -1.0e-05 [131, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 37.3000000000003, -1.0e-05 [132, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 12 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 132 runs with cutoff 5000.0])

    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
          
============= Performing 16 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 132 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 39.0000000000003, -1.0e-05 [133, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.1000000000003, -1.0e-05 [134, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.2000000000003, -1.0e-05 [135, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.3000000000003, -1.0e-05 [136, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.4000000000003, -1.0e-05 [137, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.5000000000003, -1.0e-05 [138, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.6000000000003, -1.0e-05 [139, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.7000000000003, -1.0e-05 [140, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.8000000000003, -1.0e-05 [141, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 39.9000000000003, -1.0e-05 [142, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 40.0000000000003, -1.0e-05 [143, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
401/100000000, 40.1000000000003/300.0
 Same incumbent, new precision:
New Incumbent: 40.1000000000003, -1.0e-05 [144, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 40.2000000000003, -1.0e-05 [145, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 40.3000000000003, -1.0e-05 [146, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 40.4000000000003, -1.0e-05 [147, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 40.5000000000003, -1.0e-05 [148, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 16 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])

   LM for iteration 6: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 6): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 6): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 148 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 40.5000000000003, -1.0e-05 [148, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])
405/100000000, 40.5000000000003/300.0
iteration 7, flip 21, evaluation count 405
    perturb to ---> c=0.6 g=20 m=0.3 po=50 pr=1 (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.2 po=50 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=200 m=0.2 po=50 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 7, start with c=0.6 g=200 m=0.2 po=50 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->3200, evaluating ...
          -> Take improving step to neighbour c=0.6 g=200 m=0.2 po=3200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) with flip 21

          
============= Performing 1 bonus runs of state: c=0.6 g=200 m=0.2 po=3200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=200 m=0.2 po=3200 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])

    Changing m: 0.2->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 200->60, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->400, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->100, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->30, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 200->80, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->3600, evaluating ...
          -> Take improving step to neighbour c=0.6 g=200 m=0.2 po=3600 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) with flip 22

          
============= Performing 14 bonus runs of state: c=0.6 g=200 m=0.2 po=3600 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 14 bonus runs: c=0.6 g=200 m=0.2 po=3600 pr=1 (-1.0e-05 [based on 16 runs with cutoff 5000.0])

    Changing m: 0.2->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 200->60, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 200->20, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing g: 200->100, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.2->0.3, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 200->80, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 200->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
          
============= Performing 12 bonus runs of state: c=0.6 g=200 m=0.2 po=3600 pr=1 (-1.0e-05 [based on 16 runs with cutoff 5000.0]) ============ 

          -> After 12 bonus runs for LM: c=0.6 g=200 m=0.2 po=3600 pr=1 (pruned27 [based on 28 runs with cutoff 5000.0])

   LM for iteration 7: c=0.6 g=200 m=0.2 po=3600 pr=1 (pruned27 [based on 28 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 7): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 7): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 148 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=200 m=0.2 po=3600 pr=1 (pruned27 [based on 28 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])
Incumbent better, keeping it
rejecting worse c=0.6 g=200 m=0.2 po=3600 pr=1 (pruned27 [based on 28 runs with cutoff 5000.0]), going back to c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])
460/100000000, 46.0000000000004/300.0
iteration 8, flip 24, evaluation count 460
    perturb to ---> c=0.6 g=20 m=0.3 po=3200 pr=1 (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=80 m=0.3 po=3200 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.6 g=80 m=0.3 po=30 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 8, start with c=0.6 g=80 m=0.3 po=30 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
          -> Take improving step to neighbour c=0.6 g=80 m=0.3 po=200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) with flip 24

          
============= Performing 1 bonus runs of state: c=0.6 g=80 m=0.3 po=200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=80 m=0.3 po=200 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])

    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->50, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->2400, evaluating ...
          -> Take improving step to neighbour c=0.6 g=80 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0]) with flip 25

          
============= Performing 6 bonus runs of state: c=0.6 g=80 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 6 bonus runs: c=0.6 g=80 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0])

    Changing po: 2400->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->100, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing po: 2400->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->400, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 80->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->60, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 80->100, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->800, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2400->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->20, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
          
============= Performing 18 bonus runs of state: c=0.6 g=80 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0]) ============ 

501/100000000, 50.1000000000004/300.0
          -> After 18 bonus runs for LM: c=0.6 g=80 m=0.3 po=2400 pr=1 (pruned26 [based on 27 runs with cutoff 5000.0])

   LM for iteration 8: c=0.6 g=80 m=0.3 po=2400 pr=1 (pruned26 [based on 27 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 8): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 8): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 148 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=80 m=0.3 po=2400 pr=1 (pruned26 [based on 27 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])
Incumbent better, keeping it
rejecting worse c=0.6 g=80 m=0.3 po=2400 pr=1 (pruned26 [based on 27 runs with cutoff 5000.0]), going back to c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0])
511/100000000, 51.1000000000005/300.0
iteration 9, flip 27, evaluation count 511
    perturb to ---> c=0.8 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=20 m=0.3 po=2800 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.8 g=40 m=0.3 po=2800 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 9, start with c=0.8 g=40 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.8->0.6, evaluating ...
          -> Take improving step to neighbour c=0.6 g=40 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) with flip 27

          
============= Performing 1 bonus runs of state: c=0.6 g=40 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=40 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])

    Changing g: 40->20, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0]) with flip 28

          
============= Performing 1 bonus runs of state: c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 10 runs with cutoff 5000.0])

    Changing po: 2800->50, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2800->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2800->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 2800->800, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing po: 2800->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2800->3600, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 24 runs with cutoff 5000.0]) with flip 29

          
============= Performing 9 bonus runs of state: c=0.6 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 24 runs with cutoff 5000.0]) ============ 

          -> After 9 bonus runs: c=0.6 g=20 m=0.3 po=3600 pr=1 (pruned32 [based on 33 runs with cutoff 5000.0])

    Changing po: 3600->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 3600->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 3600->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 3600->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0]) with flip 30

          
============= Performing 12 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 148 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 54.7000000000005, -1.0e-05 [149, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 54.8000000000005, -1.0e-05 [150, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 54.9000000000005, -1.0e-05 [151, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.0000000000005, -1.0e-05 [152, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.1000000000005, -1.0e-05 [153, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.2000000000005, -1.0e-05 [154, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.3000000000005, -1.0e-05 [155, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.4000000000005, -1.0e-05 [156, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.5000000000005, -1.0e-05 [157, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.6000000000005, -1.0e-05 [158, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.7000000000005, -1.0e-05 [159, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 55.8000000000005, -1.0e-05 [160, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 12 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 160 runs with cutoff 5000.0])

    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing po: 100->400, evaluating ...
        -> worse: (-1.0e-05 [based on 16 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
          
============= Performing 13 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 160 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 57.2000000000005, -1.0e-05 [161, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 57.3000000000005, -1.0e-05 [162, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 57.4000000000005, -1.0e-05 [163, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 57.5000000000005, -1.0e-05 [164, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 57.6000000000005, -1.0e-05 [165, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 57.7000000000005, -1.0e-05 [166, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 57.8000000000006, -1.0e-05 [167, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 57.9000000000006, -1.0e-05 [168, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 58.0000000000006, -1.0e-05 [169, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 58.1000000000006, -1.0e-05 [170, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 58.2000000000006, -1.0e-05 [171, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 58.3000000000006, -1.0e-05 [172, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 58.4000000000006, -1.0e-05 [173, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 13 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 173 runs with cutoff 5000.0])

   LM for iteration 9: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 173 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 9): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 9): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 173 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 173 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 173 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 58.4000000000006, -1.0e-05 [173, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 173 runs with cutoff 5000.0])
584/100000000, 58.4000000000006/300.0
iteration 10, flip 32, evaluation count 584
    perturb to ---> c=0.6 g=20 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=200 pr=1 (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    perturb to ---> c=0.7 g=20 m=0.3 po=200 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 10, start with c=0.7 g=20 m=0.3 po=200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->2800, evaluating ...
          -> Take improving step to neighbour c=0.7 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) with flip 32

          
============= Performing 2 bonus runs of state: c=0.7 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 2 bonus runs: c=0.7 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 4 runs with cutoff 5000.0])

    Changing po: 2800->400, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2800->50, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2800->800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2800->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.7->0.6, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 11 runs with cutoff 5000.0]) with flip 33

          
============= Performing 7 bonus runs of state: c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 11 runs with cutoff 5000.0]) ============ 

601/100000000, 60.1000000000006/300.0
          -> After 7 bonus runs: c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 18 runs with cutoff 5000.0])

    Changing po: 2800->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 2800->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 2800->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2800->400, evaluating ...
        -> worse: (-1.0e-05 [based on 17 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2800->800, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 2800->30, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 2800->3600, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=3600 pr=1 (pruned33 [based on 34 runs with cutoff 5000.0]) with flip 34

          
============= Performing 10 bonus runs of state: c=0.6 g=20 m=0.3 po=3600 pr=1 (pruned33 [based on 34 runs with cutoff 5000.0]) ============ 

          -> After 10 bonus runs: c=0.6 g=20 m=0.3 po=3600 pr=1 (pruned43 [based on 44 runs with cutoff 5000.0])

    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 3600->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 3600->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 3600->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 3600->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 173 runs with cutoff 5000.0]) with flip 35

          
============= Performing 6 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 173 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 63.0000000000006, -1.0e-05 [174, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 63.1000000000006, -1.0e-05 [175, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 63.2000000000006, -1.0e-05 [176, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 63.3000000000006, -1.0e-05 [177, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 63.4000000000006, -1.0e-05 [178, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 63.5000000000006, -1.0e-05 [179, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 6 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 179 runs with cutoff 5000.0])

    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 15 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 8 runs with cutoff 5000.0])
          
============= Performing 12 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 179 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 64.8000000000006, -1.0e-05 [180, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 64.9000000000006, -1.0e-05 [181, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.0000000000006, -1.0e-05 [182, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.1000000000006, -1.0e-05 [183, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.2000000000006, -1.0e-05 [184, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.3000000000006, -1.0e-05 [185, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.4000000000006, -1.0e-05 [186, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.5000000000006, -1.0e-05 [187, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.6000000000005, -1.0e-05 [188, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.7000000000005, -1.0e-05 [189, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.8000000000005, -1.0e-05 [190, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 65.9000000000005, -1.0e-05 [191, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 12 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 191 runs with cutoff 5000.0])

   LM for iteration 10: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 191 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 10): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 10): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 191 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 191 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 191 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 65.9000000000005, -1.0e-05 [191, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 191 runs with cutoff 5000.0])
659/100000000, 65.9000000000005/300.0
iteration 11, flip 37, evaluation count 659
    perturb to ---> c=0.6 g=200 m=0.3 po=100 pr=1 (-1.0e-05 [based on 8 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=200 m=0.2 po=100 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=100 m=0.2 po=100 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 11, start with c=0.6 g=100 m=0.2 po=100 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
          -> Take improving step to neighbour c=0.6 g=100 m=0.2 po=2400 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) with flip 37

          
============= Performing 1 bonus runs of state: c=0.6 g=100 m=0.2 po=2400 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=100 m=0.2 po=2400 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])

    Changing po: 2400->30, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.2->0.3, evaluating ...
          -> Take improving step to neighbour c=0.6 g=100 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) with flip 38

          
============= Performing 2 bonus runs of state: c=0.6 g=100 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 2 bonus runs: c=0.6 g=100 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 4 runs with cutoff 5000.0])

    Changing po: 2400->400, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 100->40, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2400->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 100->20, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0]) with flip 39

          
============= Performing 5 bonus runs of state: c=0.6 g=20 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0]) ============ 

          -> After 5 bonus runs: c=0.6 g=20 m=0.3 po=2400 pr=1 (-1.0e-05 [based on 14 runs with cutoff 5000.0])

    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->30, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 2400->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2400->3600, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=3600 pr=1 (pruned44 [based on 45 runs with cutoff 5000.0]) with flip 40

          
============= Performing 6 bonus runs of state: c=0.6 g=20 m=0.3 po=3600 pr=1 (pruned44 [based on 45 runs with cutoff 5000.0]) ============ 

          -> After 6 bonus runs: c=0.6 g=20 m=0.3 po=3600 pr=1 (pruned50 [based on 51 runs with cutoff 5000.0])

    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 3600->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 191 runs with cutoff 5000.0]) with flip 41

          
============= Performing 3 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 191 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 69.2000000000003, -1.0e-05 [192, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 69.3000000000003, -1.0e-05 [193, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 69.4000000000003, -1.0e-05 [194, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 3 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 194 runs with cutoff 5000.0])

    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 100->400, evaluating ...
702/100000000, 70.2000000000003/300.0
        -> worse: (-1.0e-05 [based on 18 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 16 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 20 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 9 runs with cutoff 5000.0])
          
============= Performing 19 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 194 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 71.4000000000002, -1.0e-05 [195, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 71.5000000000002, -1.0e-05 [196, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 71.6000000000002, -1.0e-05 [197, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 71.7000000000002, -1.0e-05 [198, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 71.8000000000002, -1.0e-05 [199, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 71.9000000000002, -1.0e-05 [200, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.0000000000002, -1.0e-05 [201, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.1000000000002, -1.0e-05 [202, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.2000000000002, -1.0e-05 [203, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.3000000000002, -1.0e-05 [204, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.4000000000002, -1.0e-05 [205, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.5000000000002, -1.0e-05 [206, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.6000000000002, -1.0e-05 [207, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.7000000000001, -1.0e-05 [208, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.8000000000001, -1.0e-05 [209, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 72.9000000000001, -1.0e-05 [210, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 73.0000000000001, -1.0e-05 [211, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 73.1000000000001, -1.0e-05 [212, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 73.2000000000001, -1.0e-05 [213, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 19 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 213 runs with cutoff 5000.0])

   LM for iteration 11: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 213 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 11): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 11): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 213 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 213 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 213 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 73.2000000000001, -1.0e-05 [213, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 213 runs with cutoff 5000.0])
732/100000000, 73.2000000000001/300.0
iteration 12, flip 43, evaluation count 732
    perturb to ---> c=0.6 g=20 m=0.3 po=30 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 213 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=80 m=0.3 po=100 pr=1 (-1.0e-05 [based on 10 runs with cutoff 5000.0])
   BLS in iteration 12, start with c=0.6 g=80 m=0.3 po=100 pr=1 (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing g: 80->200, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 80->60, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 80->20, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 213 runs with cutoff 5000.0]) with flip 43

          
============= Performing 8 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 213 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 74.2000000000001, -1.0e-05 [214, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 74.3000000000001, -1.0e-05 [215, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 74.4, -1.0e-05 [216, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 74.5, -1.0e-05 [217, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 74.6, -1.0e-05 [218, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 74.7, -1.0e-05 [219, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 74.8, -1.0e-05 [220, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 74.9, -1.0e-05 [221, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 8 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 221 runs with cutoff 5000.0])

    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->400, evaluating ...
        -> worse: (-1.0e-05 [based on 19 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (pruned52 [based on 53 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 16 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 21 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 17 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 10 runs with cutoff 5000.0])
          
============= Performing 20 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 221 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 76.9999999999999, -1.0e-05 [222, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.0999999999999, -1.0e-05 [223, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.1999999999999, -1.0e-05 [224, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.2999999999999, -1.0e-05 [225, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.3999999999999, -1.0e-05 [226, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.4999999999999, -1.0e-05 [227, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.5999999999999, -1.0e-05 [228, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.6999999999999, -1.0e-05 [229, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.7999999999999, -1.0e-05 [230, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.8999999999998, -1.0e-05 [231, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 77.9999999999998, -1.0e-05 [232, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.0999999999998, -1.0e-05 [233, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.1999999999998, -1.0e-05 [234, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.2999999999998, -1.0e-05 [235, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.3999999999998, -1.0e-05 [236, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.4999999999998, -1.0e-05 [237, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.5999999999998, -1.0e-05 [238, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.6999999999998, -1.0e-05 [239, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.7999999999998, -1.0e-05 [240, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 78.8999999999998, -1.0e-05 [241, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 20 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 241 runs with cutoff 5000.0])

   LM for iteration 12: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 241 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 12): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 12): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 241 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 241 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 241 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 78.8999999999998, -1.0e-05 [241, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 241 runs with cutoff 5000.0])
789/100000000, 78.8999999999998/300.0
iteration 13, flip 45, evaluation count 789
    perturb to ---> c=0.6 g=20 m=0.3 po=50 pr=1 (-1.0e-05 [based on 10 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=20 m=0.3 po=50 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.8 g=20 m=0.3 po=1200 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 13, start with c=0.8 g=20 m=0.3 po=1200 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 1200->30, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=30 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) with flip 45

          
============= Performing 1 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.8 g=20 m=0.3 po=30 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])

    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0]) with flip 46

          
============= Performing 5 bonus runs of state: c=0.8 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 5 bonus runs: c=0.8 g=20 m=0.3 po=3600 pr=1 (-1.0e-05 [based on 8 runs with cutoff 5000.0])

    Changing m: 0.3->0.2, evaluating ...
803/100000000, 80.2999999999997/300.0
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->200, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 3600->100, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 10 runs with cutoff 5000.0]) with flip 47

          
============= Performing 9 bonus runs of state: c=0.8 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 10 runs with cutoff 5000.0]) ============ 

          -> After 9 bonus runs: c=0.8 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 19 runs with cutoff 5000.0])

    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.8->0.6, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 241 runs with cutoff 5000.0]) with flip 48

          
============= Performing 11 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 241 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 83.1999999999995, -1.0e-05 [242, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 83.2999999999995, -1.0e-05 [243, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 83.3999999999995, -1.0e-05 [244, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 83.4999999999995, -1.0e-05 [245, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 83.5999999999995, -1.0e-05 [246, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 83.6999999999995, -1.0e-05 [247, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 83.7999999999995, -1.0e-05 [248, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 83.8999999999995, -1.0e-05 [249, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 83.9999999999995, -1.0e-05 [250, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 84.0999999999995, -1.0e-05 [251, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 84.1999999999995, -1.0e-05 [252, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 11 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 252 runs with cutoff 5000.0])

    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 18 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 17 runs with cutoff 5000.0])
    Changing po: 100->400, evaluating ...
        -> worse: (-1.0e-05 [based on 20 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 22 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (pruned53 [based on 54 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 11 runs with cutoff 5000.0])
          
============= Performing 21 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 252 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 86.3999999999994, -1.0e-05 [253, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 86.4999999999994, -1.0e-05 [254, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 86.5999999999994, -1.0e-05 [255, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 86.6999999999993, -1.0e-05 [256, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 86.7999999999993, -1.0e-05 [257, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 86.8999999999993, -1.0e-05 [258, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 86.9999999999993, -1.0e-05 [259, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.0999999999993, -1.0e-05 [260, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.1999999999993, -1.0e-05 [261, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.2999999999993, -1.0e-05 [262, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.3999999999993, -1.0e-05 [263, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.4999999999993, -1.0e-05 [264, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.5999999999993, -1.0e-05 [265, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.6999999999993, -1.0e-05 [266, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.7999999999993, -1.0e-05 [267, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.8999999999993, -1.0e-05 [268, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 87.9999999999993, -1.0e-05 [269, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 88.0999999999993, -1.0e-05 [270, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 88.1999999999993, -1.0e-05 [271, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 88.2999999999993, -1.0e-05 [272, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 88.3999999999993, -1.0e-05 [273, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 21 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 273 runs with cutoff 5000.0])

   LM for iteration 13: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 273 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 13): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 13): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 273 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 273 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 273 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 88.3999999999993, -1.0e-05 [273, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 273 runs with cutoff 5000.0])
884/100000000, 88.3999999999993/300.0
iteration 14, flip 50, evaluation count 884
    perturb to ---> c=0.6 g=20 m=0.3 po=2000 pr=1 (-1.0e-05 [based on 11 runs with cutoff 5000.0])
    perturb to ---> c=0.7 g=20 m=0.3 po=2000 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.7 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])
   BLS in iteration 14, start with c=0.7 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 400->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.7->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->2800, evaluating ...
          -> Take improving step to neighbour c=0.7 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0]) with flip 50

          
============= Performing 4 bonus runs of state: c=0.7 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 5 runs with cutoff 5000.0]) ============ 

          -> After 4 bonus runs: c=0.7 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 9 runs with cutoff 5000.0])

    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2800->200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 2800->30, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.7->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2800->800, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2800->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 2800->50, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.7->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.7->0.6, evaluating ...
904/100000000, 90.3999999999991/300.0
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 22 runs with cutoff 5000.0]) with flip 51

          
============= Performing 11 bonus runs of state: c=0.6 g=20 m=0.3 po=2800 pr=1 (-1.0e-05 [based on 22 runs with cutoff 5000.0]) ============ 

          -> After 11 bonus runs: c=0.6 g=20 m=0.3 po=2800 pr=1 (pruned32 [based on 33 runs with cutoff 5000.0])

    Changing po: 2800->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing po: 2800->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 273 runs with cutoff 5000.0]) with flip 52

          
============= Performing 3 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 273 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 91.8999999999991, -1.0e-05 [274, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 91.999999999999, -1.0e-05 [275, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 92.099999999999, -1.0e-05 [276, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 3 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 276 runs with cutoff 5000.0])

    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 18 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->400, evaluating ...
        -> worse: (-1.0e-05 [based on 21 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 21 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 19 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (pruned54 [based on 55 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 12 runs with cutoff 5000.0])
          
============= Performing 21 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 276 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 94.2999999999989, -1.0e-05 [277, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 94.3999999999989, -1.0e-05 [278, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 94.4999999999989, -1.0e-05 [279, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 94.5999999999989, -1.0e-05 [280, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 94.6999999999989, -1.0e-05 [281, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 94.7999999999989, -1.0e-05 [282, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 94.8999999999989, -1.0e-05 [283, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 94.9999999999989, -1.0e-05 [284, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.0999999999989, -1.0e-05 [285, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.1999999999989, -1.0e-05 [286, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.2999999999989, -1.0e-05 [287, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.3999999999989, -1.0e-05 [288, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.4999999999988, -1.0e-05 [289, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.5999999999988, -1.0e-05 [290, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.6999999999988, -1.0e-05 [291, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.7999999999988, -1.0e-05 [292, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.8999999999988, -1.0e-05 [293, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 95.9999999999988, -1.0e-05 [294, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 96.0999999999988, -1.0e-05 [295, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 96.1999999999988, -1.0e-05 [296, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 96.2999999999988, -1.0e-05 [297, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 21 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 297 runs with cutoff 5000.0])

   LM for iteration 14: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 297 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 14): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 14): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 297 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 297 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 297 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 96.2999999999988, -1.0e-05 [297, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 297 runs with cutoff 5000.0])
963/100000000, 96.2999999999988/300.0
iteration 15, flip 54, evaluation count 963
    perturb to ---> c=0.6 g=20 m=0.3 po=800 pr=1 (-1.0e-05 [based on 12 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=2800 pr=1 (pruned33 [based on 34 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=50 pr=1 (-1.0e-05 [based on 12 runs with cutoff 5000.0])
   BLS in iteration 15, start with c=0.6 g=20 m=0.3 po=50 pr=1 (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->3200, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=3200 pr=1 (-1.0e-05 [based on 13 runs with cutoff 5000.0]) with flip 54

          
============= Performing 4 bonus runs of state: c=0.6 g=20 m=0.3 po=3200 pr=1 (-1.0e-05 [based on 13 runs with cutoff 5000.0]) ============ 

          -> After 4 bonus runs: c=0.6 g=20 m=0.3 po=3200 pr=1 (-1.0e-05 [based on 17 runs with cutoff 5000.0])

    Changing po: 3200->400, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 21 runs with cutoff 5000.0]) with flip 55

          
============= Performing 1 bonus runs of state: c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 21 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 22 runs with cutoff 5000.0])

    Changing po: 400->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 297 runs with cutoff 5000.0]) with flip 56

          
============= Performing 1 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 297 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 97.5999999999987, -1.0e-05 [298, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 1 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 298 runs with cutoff 5000.0])

    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 19 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (pruned55 [based on 56 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 22 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (pruned34 [based on 35 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (-1.0e-05 [based on 15 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (-1.0e-05 [based on 20 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (-1.0e-05 [based on 13 runs with cutoff 5000.0])
          
============= Performing 20 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 298 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 99.6999999999986, -1.0e-05 [299, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 99.7999999999986, -1.0e-05 [300, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 99.8999999999986, -1.0e-05 [301, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 99.9999999999986, -1.0e-05 [302, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.099999999999, -1.0e-05 [303, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.199999999999, -1.0e-05 [304, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.299999999999, -1.0e-05 [305, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.399999999999, -1.0e-05 [306, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
1005/100000000, 100.499999999999/300.0
 Same incumbent, new precision:
New Incumbent: 100.499999999999, -1.0e-05 [307, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.599999999999, -1.0e-05 [308, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.699999999999, -1.0e-05 [309, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.799999999999, -1.0e-05 [310, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.899999999999, -1.0e-05 [311, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 100.999999999999, -1.0e-05 [312, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 101.099999999999, -1.0e-05 [313, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 101.199999999999, -1.0e-05 [314, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 101.299999999999, -1.0e-05 [315, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 101.399999999999, -1.0e-05 [316, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 101.499999999999, -1.0e-05 [317, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 101.599999999999, -1.0e-05 [318, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 20 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 318 runs with cutoff 5000.0])

   LM for iteration 15: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 318 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 15): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 15): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 318 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 318 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 318 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 101.599999999999, -1.0e-05 [318, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 318 runs with cutoff 5000.0])
1016/100000000, 101.599999999999/300.0
iteration 16, flip 58, evaluation count 1016
    perturb to ---> c=0.6 g=20 m=0.3 po=400 pr=1 (-1.0e-05 [based on 23 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=40 m=0.3 po=400 pr=1 (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=80 m=0.3 po=400 pr=1 (-1.0e-05 [based on 2 runs with cutoff 5000.0])
   BLS in iteration 16, start with c=0.6 g=80 m=0.3 po=400 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 80->200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->30, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->100, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=80 m=0.3 po=100 pr=1 (-1.0e-05 [based on 15 runs with cutoff 5000.0]) with flip 58

          
============= Performing 5 bonus runs of state: c=0.6 g=80 m=0.3 po=100 pr=1 (-1.0e-05 [based on 15 runs with cutoff 5000.0]) ============ 

          -> After 5 bonus runs: c=0.6 g=80 m=0.3 po=100 pr=1 (-1.0e-05 [based on 20 runs with cutoff 5000.0])

    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 80->40, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing g: 80->100, evaluating ...
          -> Take improving step to neighbour c=0.6 g=100 m=0.3 po=100 pr=1 (-1.0e-05 [based on 21 runs with cutoff 5000.0]) with flip 59

          
============= Performing 6 bonus runs of state: c=0.6 g=100 m=0.3 po=100 pr=1 (-1.0e-05 [based on 21 runs with cutoff 5000.0]) ============ 

          -> After 6 bonus runs: c=0.6 g=100 m=0.3 po=100 pr=1 (pruned26 [based on 27 runs with cutoff 5000.0])

    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 6 runs with cutoff 5000.0])
    Changing g: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 100->60, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing g: 100->20, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 318 runs with cutoff 5000.0]) with flip 60

          
============= Performing 19 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 318 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 105.899999999998, -1.0e-05 [319, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 105.999999999998, -1.0e-05 [320, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.099999999998, -1.0e-05 [321, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.199999999998, -1.0e-05 [322, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.299999999998, -1.0e-05 [323, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.399999999998, -1.0e-05 [324, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.499999999998, -1.0e-05 [325, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.599999999998, -1.0e-05 [326, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.699999999998, -1.0e-05 [327, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.799999999998, -1.0e-05 [328, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.899999999998, -1.0e-05 [329, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 106.999999999998, -1.0e-05 [330, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 107.099999999998, -1.0e-05 [331, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 107.199999999998, -1.0e-05 [332, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 107.299999999998, -1.0e-05 [333, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 107.399999999998, -1.0e-05 [334, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 107.499999999998, -1.0e-05 [335, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 107.599999999998, -1.0e-05 [336, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 107.699999999998, -1.0e-05 [337, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 19 bonus runs: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 337 runs with cutoff 5000.0])

    Changing po: 100->400, evaluating ...
        -> worse: (-1.0e-05 [based on 24 runs with cutoff 5000.0])
    Changing po: 100->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->200, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->800, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 19 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 23 runs with cutoff 5000.0])
    Changing po: 100->2400, evaluating ...
        -> worse: (-1.0e-05 [based on 20 runs with cutoff 5000.0])
    Changing po: 100->50, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->3600, evaluating ...
        -> worse: (pruned56 [based on 57 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    Changing po: 100->2800, evaluating ...
        -> worse: (pruned35 [based on 36 runs with cutoff 5000.0])
          
============= Performing 18 bonus runs of state: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 337 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 109.599999999998, -1.0e-05 [338, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 109.699999999998, -1.0e-05 [339, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 109.799999999998, -1.0e-05 [340, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 109.899999999998, -1.0e-05 [341, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 109.999999999998, -1.0e-05 [342, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.099999999998, -1.0e-05 [343, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.199999999998, -1.0e-05 [344, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.299999999998, -1.0e-05 [345, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.399999999998, -1.0e-05 [346, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.499999999998, -1.0e-05 [347, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
1106/100000000, 110.599999999998/300.0
 Same incumbent, new precision:
New Incumbent: 110.599999999998, -1.0e-05 [348, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.699999999998, -1.0e-05 [349, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.799999999998, -1.0e-05 [350, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.899999999998, -1.0e-05 [351, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 110.999999999998, -1.0e-05 [352, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 111.099999999998, -1.0e-05 [353, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 111.199999999998, -1.0e-05 [354, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
 Same incumbent, new precision:
New Incumbent: 111.299999999998, -1.0e-05 [355, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
          -> After 18 bonus runs for LM: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 355 runs with cutoff 5000.0])

   LM for iteration 16: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 355 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 16): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 16): pr=1, m=0.3, c=0.6, g=20, po=100
==================================================================
Training quality of this incumbent parameter configuration: -1.0e-05, based on 355 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 355 runs with cutoff 5000.0])
c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 355 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 111.299999999998, -1.0e-05 [355, 5000.0]. With state c=0.6, g=20, m=0.3, po=100, pr=1
same state as last ILS: c=0.6 g=20 m=0.3 po=100 pr=1 (-1.0e-05 [based on 355 runs with cutoff 5000.0])
1113/100000000, 111.299999999998/300.0
iteration 17, flip 62, evaluation count 1113
    perturb to ---> c=0.6 g=20 m=0.3 po=1200 pr=1 (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=200 pr=1 (-1.0e-05 [based on 14 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=80 m=0.3 po=200 pr=1 (-1.0e-05 [based on 3 runs with cutoff 5000.0])
   BLS in iteration 17, start with c=0.6 g=80 m=0.3 po=200 pr=1 (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->2400, evaluating ...
          -> Take improving step to neighbour c=0.6 g=80 m=0.3 po=2400 pr=1 (pruned26 [based on 27 runs with cutoff 5000.0]) with flip 62

          
============= Performing 2 bonus runs of state: c=0.6 g=80 m=0.3 po=2400 pr=1 (pruned26 [based on 27 runs with cutoff 5000.0]) ============ 

          -> After 2 bonus runs: c=0.6 g=80 m=0.3 po=2400 pr=1 (pruned28 [based on 29 runs with cutoff 5000.0])

    Changing c: 0.6->0.9, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2400->3600, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing po: 2400->400, evaluating ...
        -> worse: (-1.0e-05 [based on 5 runs with cutoff 5000.0])
    Changing g: 80->100, evaluating ...
        -> worse: (-1.0e-05 [based on 7 runs with cutoff 5000.0])
    Changing po: 2400->4000, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing c: 0.6->0.7, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 80->60, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2400->1200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 2400->800, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 2400->100, evaluating ...
        -> worse: (-1.0e-05 [based on 22 runs with cutoff 5000.0])
    Changing g: 80->40, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing g: 80->20, evaluating ...
        -> worse: (-1.0e-05 [based on 21 runs with cutoff 5000.0])
    Changing po: 2400->50, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2400->2800, evaluating ...
        -> worse: (-1.0e-05 [based on 4 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2400->1600, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 2400->30, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing po: 2400->3200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing c: 0.6->0.8, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing po: 2400->2000, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
    Changing g: 80->200, evaluating ...
        -> worse: (-1.0e-05 [based on 3 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (-1.0e-05 [based on 2 runs with cutoff 5000.0])
          
============= Performing 22 bonus runs of state: c=0.6 g=80 m=0.3 po=2400 pr=1 (pruned28 [based on 29 runs with cutoff 5000.0]) ============ 

