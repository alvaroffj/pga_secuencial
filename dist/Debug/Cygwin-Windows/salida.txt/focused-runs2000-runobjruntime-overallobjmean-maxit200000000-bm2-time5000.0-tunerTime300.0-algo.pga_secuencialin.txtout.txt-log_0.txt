Call: /usr/bin/ruby ../scripts/param_ils_2_3_run.rb "-numRun" "0" "-scenariofile" "escenario.txt" "-validN" "5"


seed: 1234
algo: ./pga_secuencial in.txt out.txt
tunerTimeout: 300.0
maxEvals: 100000000
run_obj: runtime
overall_obj: mean
instance_file: in.txt
test_instance_file: in.txt
N: 2000
cutoff_time: 5000.0
cutoff_length: 200000
R: 10
pertubation_strength_basic: 
pertubation_strength_scaling: false
p_restart: 0.01
Run 1
Level 
========================================================
Starting ILS for level 1, i.e. a limit of N=2000, and cutoff time=5000.0.
Current CPU time = 0, this run goes until 300.0 
========================================================
New Incumbent: 0, 100000000 [0, 0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 0.24, 0.24 [1, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
        -> Worse random: c=0.6 g=80 m=0.3 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=100 m=0.1 po=2400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.6 g=40 m=0.2 po=2800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.7 g=80 m=0.3 po=3600 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=60 m=0.3 po=2800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=20 m=0.1 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=40 m=0.3 po=100 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=80 m=0.1 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.7 g=60 m=0.1 po=2400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.6 g=20 m=0.3 po=200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
   BLS in iteration 1, start with c=0.9 g=20 m=0.1 po=30 pr=1 (0.24 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.3 po=30 pr=1 (0.24 [based on 1 runs with cutoff 5000.0]) with flip 1

          
============= Performing 2 bonus runs of state: c=0.9 g=20 m=0.3 po=30 pr=1 (0.24 [based on 1 runs with cutoff 5000.0]) ============ 

State wants more detail (1+1) than incumbent (1), doing incumbent first:
c=0.9 g=20 m=0.3 po=30 pr=1 (0.24 [based on 1 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.24 [based on 1 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 3.47, 0.24 [2, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (2+1) than incumbent (2), doing incumbent first:
c=0.9 g=20 m=0.3 po=30 pr=1 (0.245 [based on 2 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.24 [based on 2 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 3.94, 0.233333333333333 [3, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 2 bonus runs: c=0.9 g=20 m=0.3 po=30 pr=1 (0.243333333333333 [based on 3 runs with cutoff 5000.0])

    Changing po: 30->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (0.24 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=30 pr=1 (0.236666666666667 [based on 3 runs with cutoff 5000.0]) with flip 2

          
============= Performing 19 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.236666666666667 [based on 3 runs with cutoff 5000.0]) ============ 

State wants more detail (3+1) than incumbent (3), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.236666666666667 [based on 3 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.233333333333333 [based on 3 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 9.61, 0.2325 [4, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (4+1) than incumbent (4), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.2375 [based on 4 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.2325 [based on 4 runs with cutoff 5000.0])
41/100000000, 10.07/300.0
 Same incumbent, new precision:
New Incumbent: 10.07, 0.23 [5, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (5+1) than incumbent (5), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.24 [based on 5 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.23 [based on 5 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 10.55, 0.23 [6, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (6+1) than incumbent (6), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.24 [based on 6 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.23 [based on 6 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 11.02, 0.23 [7, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (7+1) than incumbent (7), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.24 [based on 7 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.23 [based on 7 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 11.5, 0.23125 [8, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (8+1) than incumbent (8), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.24 [based on 8 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.23125 [based on 8 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 11.99, 0.233333333333333 [9, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (9+1) than incumbent (9), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.24 [based on 9 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.233333333333333 [based on 9 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 12.48, 0.235 [10, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (10+1) than incumbent (10), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.24 [based on 10 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.235 [based on 10 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 12.97, 0.236363636363636 [11, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (11+1) than incumbent (11), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.239090909090909 [based on 11 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.236363636363636 [based on 11 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 13.43, 0.235833333333333 [12, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (12+1) than incumbent (12), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.239166666666667 [based on 12 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.235833333333333 [based on 12 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 13.9, 0.235384615384615 [13, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (13+1) than incumbent (13), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.239230769230769 [based on 13 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.235384615384615 [based on 13 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 14.38, 0.235714285714286 [14, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (14+1) than incumbent (14), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.239285714285714 [based on 14 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.235714285714286 [based on 14 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 14.87, 0.236666666666667 [15, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (15+1) than incumbent (15), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.239333333333333 [based on 15 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.236666666666667 [based on 15 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 15.35, 0.236875 [16, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (16+1) than incumbent (16), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.23875 [based on 16 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.236875 [based on 16 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 15.83, 0.237647058823529 [17, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (17+1) than incumbent (17), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.238235294117647 [based on 17 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.237647058823529 [based on 17 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 16.29, 0.237222222222222 [18, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (18+1) than incumbent (18), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.237777777777778 [based on 18 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.237222222222222 [based on 18 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 16.76, 0.237368421052632 [19, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (19+1) than incumbent (19), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.237368421052632 [based on 19 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.237368421052632 [based on 19 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 17.22, 0.237 [20, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (20+1) than incumbent (20), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.2375 [based on 20 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.237 [based on 20 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 17.7, 0.237142857142857 [21, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (21+1) than incumbent (21), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.238095238095238 [based on 21 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.237142857142857 [based on 21 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 18.19, 0.237272727272727 [22, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 19 bonus runs: c=0.8 g=20 m=0.3 po=30 pr=1 (0.238181818181818 [based on 22 runs with cutoff 5000.0])

    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
83/100000000, 20.11/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
          
============= Performing 20 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.238181818181818 [based on 22 runs with cutoff 5000.0]) ============ 

State wants more detail (22+1) than incumbent (22), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.238181818181818 [based on 22 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.237272727272727 [based on 22 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 24.01, 0.260869565217391 [23, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
New inc: 0.259565217391304
New Incumbent: 24.74, 0.259565217391304 [23, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 25.48, 0.279583333333333 [24, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 26.09, 0.2928 [25, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 26.76, 0.307307692307692 [26, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 27.4, 0.31962962962963 [27, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 28.15, 0.335 [28, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 28.91, 0.349655172413793 [29, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 29.27, 0.35 [30, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 29.61, 0.349677419354839 [31, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 29.99, 0.350625 [32, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
108/100000000, 30.37/300.0
 Same incumbent, new precision:
New Incumbent: 30.37, 0.351515151515152 [33, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 30.71, 0.351176470588235 [34, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 31.06, 0.351142857142857 [35, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 31.39, 0.350555555555556 [36, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 31.76, 0.351081081081081 [37, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 32.13, 0.351578947368421 [38, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 32.5, 0.352051282051282 [39, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 32.85, 0.352 [40, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 33.15, 0.350731707317073 [41, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 33.41, 0.348571428571429 [42, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
          -> After 20 bonus runs for LM: c=0.8 g=20 m=0.3 po=30 pr=1 (0.348571428571429 [based on 42 runs with cutoff 5000.0])

   LM for iteration 1: c=0.8 g=20 m=0.3 po=30 pr=1 (0.348571428571429 [based on 42 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 1): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 1): pr=1, m=0.3, c=0.8, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.348571428571429, based on 42 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.348571428571429 [based on 42 runs with cutoff 5000.0])
c=0.8 g=20 m=0.3 po=30 pr=1 (0.348571428571429 [based on 42 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 33.41, 0.348571428571429 [42, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
117/100000000, 33.41/300.0
iteration 2, flip 4, evaluation count 117
    perturb to ---> c=0.8 g=20 m=0.3 po=1200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=20 m=0.3 po=4000 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=20 m=0.3 po=2800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
   BLS in iteration 2, start with c=0.8 g=20 m=0.3 po=2800 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 2800->4000, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=4000 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) with flip 4

          
============= Performing 1 bonus runs of state: c=0.8 g=20 m=0.3 po=4000 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.8 g=20 m=0.3 po=4000 pr=1 (pruned1 [based on 3 runs with cutoff 5000.0])

    Changing po: 4000->800, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 4000->1200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 4000->50, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=50 pr=1 (0.41 [based on 3 runs with cutoff 5000.0]) with flip 5

          
============= Performing 4 bonus runs of state: c=0.8 g=20 m=0.3 po=50 pr=1 (0.41 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 4 bonus runs: c=0.8 g=20 m=0.3 po=50 pr=1 (0.411428571428571 [based on 7 runs with cutoff 5000.0])

    Changing c: 0.8->0.6, evaluating ...
        -> worse: (0.4 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->400, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 50->2000, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 50->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->1600, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 50->3200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->30, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=30 pr=1 (0.348571428571429 [based on 42 runs with cutoff 5000.0]) with flip 6

          
============= Performing 15 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.348571428571429 [based on 42 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 39.0999999999999, 0.346279069767442 [43, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 39.3299999999999, 0.343636363636364 [44, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 39.5699999999999, 0.341333333333333 [45, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 39.8099999999999, 0.339130434782609 [46, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 40.0499999999999, 0.337021276595745 [47, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 40.2999999999999, 0.335208333333333 [48, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
138/100000000, 40.5499999999999/300.0
 Same incumbent, new precision:
New Incumbent: 40.5499999999999, 0.333469387755102 [49, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 40.7999999999999, 0.3318 [50, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 41.0499999999999, 0.330196078431373 [51, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 41.2899999999999, 0.328461538461538 [52, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 41.5299999999999, 0.326792452830189 [53, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 41.7699999999999, 0.325185185185185 [54, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 42.0199999999999, 0.323818181818182 [55, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 42.2599999999999, 0.322321428571429 [56, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 42.5, 0.320877192982456 [57, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
          -> After 15 bonus runs: c=0.8 g=20 m=0.3 po=30 pr=1 (0.320877192982456 [based on 57 runs with cutoff 5000.0])

    Changing m: 0.3->0.1, evaluating ...
        -> worse: (pruned10 [based on 11 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned3 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.8->0.6, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
          
============= Performing 12 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.320877192982456 [based on 57 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 45.90999, 0.31948275862069 [58, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 46.15999, 0.318305084745763 [59, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 46.40999, 0.317166666666667 [60, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 46.64999, 0.315901639344262 [61, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 46.90999, 0.315 [62, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 47.15999, 0.313968253968254 [63, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 47.39999, 0.3128125 [64, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 47.63999, 0.311692307692308 [65, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 47.88999, 0.310757575757576 [66, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 48.13999, 0.309850746268657 [67, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 48.36999, 0.308676470588235 [68, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 48.61999, 0.307826086956522 [69, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
          -> After 12 bonus runs for LM: c=0.8 g=20 m=0.3 po=30 pr=1 (0.307826086956522 [based on 69 runs with cutoff 5000.0])

   LM for iteration 2: c=0.8 g=20 m=0.3 po=30 pr=1 (0.307826086956522 [based on 69 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 2): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 2): pr=1, m=0.3, c=0.8, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.307826086956522, based on 69 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.307826086956522 [based on 69 runs with cutoff 5000.0])
c=0.8 g=20 m=0.3 po=30 pr=1 (0.307826086956522 [based on 69 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 48.61999, 0.307826086956522 [69, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
same state as last ILS: c=0.8 g=20 m=0.3 po=30 pr=1 (0.307826086956522 [based on 69 runs with cutoff 5000.0])
172/100000000, 48.61999/300.0
iteration 3, flip 8, evaluation count 172
    perturb to ---> c=0.8 g=200 m=0.3 po=30 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=200 m=0.3 po=1200 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.8 g=80 m=0.3 po=1200 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 3, start with c=0.8 g=80 m=0.3 po=1200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
          -> Take improving step to neighbour c=0.9 g=80 m=0.3 po=1200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) with flip 8

          
============= Performing 1 bonus runs of state: c=0.9 g=80 m=0.3 po=1200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.9 g=80 m=0.3 po=1200 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])

    Changing po: 1200->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 1200->50, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 1200->3600, evaluating ...
177/100000000, 50.96999/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 1200->2400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 1200->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 1200->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 1200->30, evaluating ...
          -> Take improving step to neighbour c=0.9 g=80 m=0.3 po=30 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) with flip 9

          
============= Performing 9 bonus runs of state: c=0.9 g=80 m=0.3 po=30 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 9 bonus runs: c=0.9 g=80 m=0.3 po=30 pr=1 (pruned5 [based on 11 runs with cutoff 5000.0])

    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->40, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 80->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->20, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.3 po=30 pr=1 (0.4 [based on 11 runs with cutoff 5000.0]) with flip 10

          
============= Performing 10 bonus runs of state: c=0.9 g=20 m=0.3 po=30 pr=1 (0.4 [based on 11 runs with cutoff 5000.0]) ============ 

201/100000000, 61.25998/300.0
          -> After 10 bonus runs: c=0.9 g=20 m=0.3 po=30 pr=1 (0.381428571428571 [based on 21 runs with cutoff 5000.0])

    Changing m: 0.3->0.1, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=30 pr=1 (0.260869565217391 [based on 23 runs with cutoff 5000.0]) with flip 11

          
============= Performing 1 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.260869565217391 [based on 23 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.9 g=20 m=0.1 po=30 pr=1 (0.26625 [based on 24 runs with cutoff 5000.0])

    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned11 [based on 12 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (0.24 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
          
============= Performing 21 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.26625 [based on 24 runs with cutoff 5000.0]) ============ 

238/100000000, 71.3899699999999/300.0
          -> After 21 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.253111111111111 [based on 45 runs with cutoff 5000.0])

   LM for iteration 3: c=0.9 g=20 m=0.1 po=30 pr=1 (0.253111111111111 [based on 45 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 3): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 3): pr=1, m=0.3, c=0.8, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.307826086956522, based on 69 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.253111111111111 [based on 45 runs with cutoff 5000.0])
c=0.8 g=20 m=0.3 po=30 pr=1 (0.307826086956522 [based on 69 runs with cutoff 5000.0])
New inc: 0.248550724637681
New Incumbent: 80.6799699999999, 0.248550724637681 [69, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
LM better, change incumbent
New Incumbent: 80.6799699999999, 0.248550724637681 [69, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
   Accepting new better local optimum: c=0.9 g=20 m=0.1 po=30 pr=1 (0.248550724637681 [based on 69 runs with cutoff 5000.0])
277/100000000, 80.6799699999999/300.0
iteration 4, flip 13, evaluation count 277
    perturb to ---> c=0.9 g=20 m=0.1 po=400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=20 m=0.1 po=3600 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=20 m=0.1 po=4000 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
   BLS in iteration 4, start with c=0.9 g=20 m=0.1 po=4000 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 4000->200, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=200 pr=1 (pruned1 [based on 3 runs with cutoff 5000.0]) with flip 13

          
============= Performing 2 bonus runs of state: c=0.9 g=20 m=0.1 po=200 pr=1 (pruned1 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 2 bonus runs: c=0.9 g=20 m=0.1 po=200 pr=1 (pruned2 [based on 5 runs with cutoff 5000.0])

    Changing po: 200->2800, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
279/100000000, 81.4199599999999/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->1600, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->30, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=30 pr=1 (0.248550724637681 [based on 69 runs with cutoff 5000.0]) with flip 14

          
============= Performing 6 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.248550724637681 [based on 69 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 82.6299599999999, 0.248285714285714 [70, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 82.8699599999999, 0.248169014084507 [71, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 83.1099599999999, 0.248055555555556 [72, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 83.3599599999999, 0.248082191780822 [73, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 83.5999599999999, 0.247972972972973 [74, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 83.8399599999999, 0.247866666666667 [75, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 6 bonus runs: c=0.9 g=20 m=0.1 po=30 pr=1 (0.247866666666667 [based on 75 runs with cutoff 5000.0])

    Changing po: 30->2000, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned12 [based on 13 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned17 [based on 23 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned3 [based on 4 runs with cutoff 5000.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned4 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
          
============= Performing 19 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.247866666666667 [based on 75 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 85.5499499999999, 0.247894736842105 [76, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 85.8099499999999, 0.248051948051948 [77, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 86.0499499999999, 0.247948717948718 [78, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 86.2899499999999, 0.247848101265823 [79, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 86.5299499999999, 0.24775 [80, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 86.7799499999999, 0.247777777777778 [81, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 87.0199499999999, 0.247682926829268 [82, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 87.2499499999999, 0.247469879518072 [83, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 87.4899499999999, 0.247380952380952 [84, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 87.7199499999999, 0.247176470588235 [85, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 87.9599499999998, 0.247093023255814 [86, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 88.2199499999999, 0.247241379310345 [87, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 88.4599499999998, 0.247159090909091 [88, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 88.7099499999998, 0.247191011235955 [89, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 88.9499499999998, 0.247111111111111 [90, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 89.1899499999998, 0.247032967032967 [91, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 89.4899499999998, 0.247608695652174 [92, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 89.7399499999998, 0.247634408602151 [93, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 89.9799499999998, 0.247553191489362 [94, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 19 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.247553191489362 [based on 94 runs with cutoff 5000.0])

   LM for iteration 4: c=0.9 g=20 m=0.1 po=30 pr=1 (0.247553191489362 [based on 94 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 4): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 4): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.247553191489362, based on 94 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.247553191489362 [based on 94 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.247553191489362 [based on 94 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 89.9799499999998, 0.247553191489362 [94, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
same state as last ILS: c=0.9 g=20 m=0.1 po=30 pr=1 (0.247553191489362 [based on 94 runs with cutoff 5000.0])
314/100000000, 89.9799499999998/300.0
iteration 5, flip 16, evaluation count 314
    perturb to ---> c=0.9 g=20 m=0.1 po=50 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])
    perturb to ---> c=0.7 g=20 m=0.1 po=50 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.7 g=20 m=0.1 po=2000 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 5, start with c=0.7 g=20 m=0.1 po=2000 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 2000->800, evaluating ...
          -> Take improving step to neighbour c=0.7 g=20 m=0.1 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) with flip 16

          
============= Performing 1 bonus runs of state: c=0.7 g=20 m=0.1 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.7 g=20 m=0.1 po=800 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])

    Changing m: 0.1->0.2, evaluating ...
317/100000000, 91.4499499999998/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 800->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 800->2400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 800->30, evaluating ...
          -> Take improving step to neighbour c=0.7 g=20 m=0.1 po=30 pr=1 (pruned4 [based on 5 runs with cutoff 5000.0]) with flip 17

          
============= Performing 4 bonus runs of state: c=0.7 g=20 m=0.1 po=30 pr=1 (pruned4 [based on 5 runs with cutoff 5000.0]) ============ 

          -> After 4 bonus runs: c=0.7 g=20 m=0.1 po=30 pr=1 (0.325555555555556 [based on 9 runs with cutoff 5000.0])

    Changing c: 0.7->0.8, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.1 po=30 pr=1 (pruned12 [based on 13 runs with cutoff 5000.0]) with flip 18

          
============= Performing 1 bonus runs of state: c=0.8 g=20 m=0.1 po=30 pr=1 (pruned12 [based on 13 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.8 g=20 m=0.1 po=30 pr=1 (0.262857142857143 [based on 14 runs with cutoff 5000.0])

    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=30 pr=1 (0.307826086956522 [based on 69 runs with cutoff 5000.0]) with flip 19

          
============= Performing 8 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.307826086956522 [based on 69 runs with cutoff 5000.0]) ============ 

          -> After 8 bonus runs: c=0.8 g=20 m=0.3 po=30 pr=1 (0.300909090909091 [based on 77 runs with cutoff 5000.0])

    Changing c: 0.8->0.6, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned2 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned18 [based on 24 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned6 [based on 9 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
          
============= Performing 22 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.300909090909091 [based on 77 runs with cutoff 5000.0]) ============ 

355/100000000, 101.64993/300.0
State wants more detail (94+1) than incumbent (94), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.289787234042553 [based on 94 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.247553191489362 [based on 94 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 103.32993, 0.247473684210526 [95, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (95+1) than incumbent (95), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.289263157894737 [based on 95 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.247473684210526 [based on 95 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 103.82993, 0.247604166666667 [96, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (96+1) than incumbent (96), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.28875 [based on 96 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.247604166666667 [based on 96 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 104.31993, 0.247628865979381 [97, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (97+1) than incumbent (97), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.288144329896907 [based on 97 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.247628865979381 [based on 97 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 104.79993, 0.24765306122449 [98, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (98+1) than incumbent (98), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.28765306122449 [based on 98 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.24765306122449 [based on 98 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 105.33993, 0.248181818181818 [99, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 22 bonus runs for LM: c=0.8 g=20 m=0.3 po=30 pr=1 (0.287171717171717 [based on 99 runs with cutoff 5000.0])

   LM for iteration 5: c=0.8 g=20 m=0.3 po=30 pr=1 (0.287171717171717 [based on 99 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 5): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 5): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.248181818181818, based on 99 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.287171717171717 [based on 99 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.248181818181818 [based on 99 runs with cutoff 5000.0])
Incumbent better, keeping it
rejecting worse c=0.8 g=20 m=0.3 po=30 pr=1 (0.287171717171717 [based on 99 runs with cutoff 5000.0]), going back to c=0.9 g=20 m=0.1 po=30 pr=1 (0.248181818181818 [based on 99 runs with cutoff 5000.0])
371/100000000, 105.57993/300.0
iteration 6, flip 21, evaluation count 371
    perturb to ---> c=0.8 g=20 m=0.1 po=30 pr=1 (pruned14 [based on 15 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=20 m=0.1 po=2800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=20 m=0.1 po=200 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 6, start with c=0.8 g=20 m=0.1 po=200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->400, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.1 po=400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) with flip 21

          
============= Performing 1 bonus runs of state: c=0.8 g=20 m=0.1 po=400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.8 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])

    Changing po: 400->3200, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.1 po=3200 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) with flip 22

          
============= Performing 1 bonus runs of state: c=0.8 g=20 m=0.1 po=3200 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.8 g=20 m=0.1 po=3200 pr=1 (pruned1 [based on 3 runs with cutoff 5000.0])

    Changing po: 3200->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->2400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->50, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.1 po=50 pr=1 (0.433333333333333 [based on 3 runs with cutoff 5000.0]) with flip 23

          
============= Performing 4 bonus runs of state: c=0.8 g=20 m=0.1 po=50 pr=1 (0.433333333333333 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 4 bonus runs: c=0.8 g=20 m=0.1 po=50 pr=1 (pruned6 [based on 7 runs with cutoff 5000.0])

    Changing m: 0.1->0.3, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=50 pr=1 (pruned6 [based on 9 runs with cutoff 5000.0]) with flip 24

          
============= Performing 1 bonus runs of state: c=0.8 g=20 m=0.3 po=50 pr=1 (pruned6 [based on 9 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.8 g=20 m=0.3 po=50 pr=1 (0.41 [based on 10 runs with cutoff 5000.0])

    Changing g: 20->200, evaluating ...
385/100000000, 111.84991/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 50->3600, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 50->2400, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 50->4000, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 50->3200, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 50->200, evaluating ...
        -> worse: (pruned2 [based on 4 runs with cutoff 5000.0])
    Changing po: 50->2000, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 50->800, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 50->1200, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 50->400, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 50->1600, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 50->100, evaluating ...
        -> worse: (pruned3 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.8->0.6, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing po: 50->30, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=30 pr=1 (0.287171717171717 [based on 99 runs with cutoff 5000.0]) with flip 25

          
============= Performing 19 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.287171717171717 [based on 99 runs with cutoff 5000.0]) ============ 

State wants more detail (99+1) than incumbent (99), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.287171717171717 [based on 99 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.248181818181818 [based on 99 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 114.01991, 0.2521 [100, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (100+1) than incumbent (100), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.2898 [based on 100 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.2521 [based on 100 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 114.82991, 0.252178217821782 [101, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (101+1) than incumbent (101), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.291980198019802 [based on 101 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.252178217821782 [based on 101 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 115.68991, 0.253137254901961 [102, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (102+1) than incumbent (102), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.292647058823529 [based on 102 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.253137254901961 [based on 102 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 116.40991, 0.254174757281553 [103, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (103+1) than incumbent (103), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.292427184466019 [based on 103 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.254174757281553 [based on 103 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 117.03991, 0.255192307692308 [104, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (104+1) than incumbent (104), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.293173076923077 [based on 104 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255192307692308 [based on 104 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 117.77991, 0.256285714285714 [105, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (105+1) than incumbent (105), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.292952380952381 [based on 105 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256285714285714 [based on 105 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 118.41991, 0.257358490566038 [106, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (106+1) than incumbent (106), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.293584905660377 [based on 106 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.257358490566038 [based on 106 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 119.03991, 0.257383177570093 [107, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (107+1) than incumbent (107), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.293177570093458 [based on 107 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.257383177570093 [based on 107 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 119.53991, 0.257314814814815 [108, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (108+1) than incumbent (108), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.292777777777778 [based on 108 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.257314814814815 [based on 108 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 120.02991, 0.257155963302752 [109, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (109+1) than incumbent (109), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.292385321100917 [based on 109 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.257155963302752 [based on 109 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 120.52991, 0.257090909090909 [110, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (110+1) than incumbent (110), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.291909090909091 [based on 110 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.257090909090909 [based on 110 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 120.99991, 0.256846846846847 [111, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (111+1) than incumbent (111), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.291441441441441 [based on 111 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256846846846847 [based on 111 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 121.48991, 0.256785714285714 [112, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (112+1) than incumbent (112), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.291071428571428 [based on 112 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256785714285714 [based on 112 runs with cutoff 5000.0])
418/100000000, 122.00991/300.0
 Same incumbent, new precision:
New Incumbent: 122.00991, 0.256902654867257 [113, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (113+1) than incumbent (113), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.290619469026549 [based on 113 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256902654867257 [based on 113 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 122.48991, 0.256754385964912 [114, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (114+1) than incumbent (114), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.290175438596491 [based on 114 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256754385964912 [based on 114 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 122.96991, 0.256608695652174 [115, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (115+1) than incumbent (115), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.289739130434782 [based on 115 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256608695652174 [based on 115 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 123.44991, 0.256465517241379 [116, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (116+1) than incumbent (116), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.289310344827586 [based on 116 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256465517241379 [based on 116 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 123.92991, 0.256324786324786 [117, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (117+1) than incumbent (117), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.288803418803419 [based on 117 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256324786324786 [based on 117 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 124.39991, 0.256186440677966 [118, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 19 bonus runs: c=0.8 g=20 m=0.3 po=30 pr=1 (0.288474576271186 [based on 118 runs with cutoff 5000.0])

    Changing g: 20->60, evaluating ...
        -> worse: (pruned2 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.8->0.6, evaluating ...
        -> worse: (pruned3 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned20 [based on 25 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (pruned15 [based on 16 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned3 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned2 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (pruned3 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned2 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
          
============= Performing 11 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.288474576271186 [based on 118 runs with cutoff 5000.0]) ============ 

State wants more detail (118+1) than incumbent (118), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.288474576271186 [based on 118 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.256186440677966 [based on 118 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 125.54989, 0.255966386554622 [119, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (119+1) than incumbent (119), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.287983193277311 [based on 119 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255966386554622 [based on 119 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 126.01989, 0.255833333333333 [120, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (120+1) than incumbent (120), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.287583333333333 [based on 120 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255833333333333 [based on 120 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 126.50989, 0.255785123966942 [121, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (121+1) than incumbent (121), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.287190082644628 [based on 121 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255785123966942 [based on 121 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 126.98989, 0.255655737704918 [122, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (122+1) than incumbent (122), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.28672131147541 [based on 122 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255655737704918 [based on 122 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 127.45989, 0.255528455284553 [123, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (123+1) than incumbent (123), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.286260162601626 [based on 123 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255528455284553 [based on 123 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 127.92989, 0.255403225806452 [124, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (124+1) than incumbent (124), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.285887096774193 [based on 124 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255403225806452 [based on 124 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 128.41989, 0.25536 [125, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (125+1) than incumbent (125), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.2856 [based on 125 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.25536 [based on 125 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 128.90989, 0.255238095238095 [126, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (126+1) than incumbent (126), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.285238095238095 [based on 126 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255238095238095 [based on 126 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 129.40989, 0.255275590551181 [127, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (127+1) than incumbent (127), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.284803149606299 [based on 127 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255275590551181 [based on 127 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 129.87989, 0.25515625 [128, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
State wants more detail (128+1) than incumbent (128), doing incumbent first:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.28453125 [based on 128 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.25515625 [based on 128 runs with cutoff 5000.0])
 Same incumbent, new precision:
New Incumbent: 130.36989, 0.255038759689922 [129, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 11 bonus runs for LM: c=0.8 g=20 m=0.3 po=30 pr=1 (0.284108527131783 [based on 129 runs with cutoff 5000.0])

   LM for iteration 6: c=0.8 g=20 m=0.3 po=30 pr=1 (0.284108527131783 [based on 129 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 6): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 6): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.255038759689922, based on 129 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.284108527131783 [based on 129 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.255038759689922 [based on 129 runs with cutoff 5000.0])
Incumbent better, keeping it
rejecting worse c=0.8 g=20 m=0.3 po=30 pr=1 (0.284108527131783 [based on 129 runs with cutoff 5000.0]), going back to c=0.9 g=20 m=0.1 po=30 pr=1 (0.255038759689922 [based on 129 runs with cutoff 5000.0])
455/100000000, 130.59989/300.0
iteration 7, flip 27, evaluation count 455
    perturb to ---> c=0.9 g=20 m=0.1 po=50 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=20 m=0.1 po=200 pr=1 (pruned1 [based on 6 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=20 m=0.2 po=200 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 7, start with c=0.9 g=20 m=0.2 po=200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->30, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.2 po=30 pr=1 (pruned2 [based on 3 runs with cutoff 5000.0]) with flip 27

          
============= Performing 1 bonus runs of state: c=0.9 g=20 m=0.2 po=30 pr=1 (pruned2 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.9 g=20 m=0.2 po=30 pr=1 (0.2725 [based on 4 runs with cutoff 5000.0])

    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
459/100000000, 132.05989/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.2->0.3, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.3 po=30 pr=1 (pruned20 [based on 25 runs with cutoff 5000.0]) with flip 28

          
============= Performing 9 bonus runs of state: c=0.9 g=20 m=0.3 po=30 pr=1 (pruned20 [based on 25 runs with cutoff 5000.0]) ============ 

          -> After 9 bonus runs: c=0.9 g=20 m=0.3 po=30 pr=1 (0.430294117647059 [based on 34 runs with cutoff 5000.0])

    Changing po: 30->1200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
482/100000000, 142.28989/300.0
        -> worse: (pruned8 [based on 9 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=30 pr=1 (0.255038759689922 [based on 129 runs with cutoff 5000.0]) with flip 29

          
============= Performing 11 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.255038759689922 [based on 129 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 144.07989, 0.255923076923077 [130, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 144.45989, 0.256870229007634 [131, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 144.88989, 0.258181818181818 [132, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 145.33989, 0.259624060150376 [133, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 145.70989, 0.26044776119403 [134, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 146.12989, 0.26162962962963 [135, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 146.55989, 0.262867647058823 [136, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 146.93989, 0.263722627737226 [137, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 147.31989, 0.264565217391304 [138, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 147.82989, 0.266330935251799 [139, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 148.20989, 0.267142857142857 [140, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 11 bonus runs: c=0.9 g=20 m=0.1 po=30 pr=1 (0.267142857142857 [based on 140 runs with cutoff 5000.0])

    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned9 [based on 11 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned4 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned2 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned16 [based on 17 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
          
============= Performing 21 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.267142857142857 [based on 140 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 149.48987, 0.270425531914894 [141, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 150.02987, 0.272323943661972 [142, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 150.71987, 0.275244755244755 [143, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 151.14987, 0.276319444444444 [144, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 151.51987, 0.276965517241379 [145, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 151.92987, 0.277876712328767 [146, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
506/100000000, 152.49987/300.0
 Same incumbent, new precision:
New Incumbent: 152.49987, 0.279863945578231 [147, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 152.96987, 0.281148648648649 [148, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 153.44987, 0.28248322147651 [149, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 153.87987, 0.283466666666667 [150, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 154.45987, 0.285430463576159 [151, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 154.80987, 0.285855263157895 [152, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 155.39987, 0.287843137254902 [153, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 155.78987, 0.288506493506493 [154, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 156.12987, 0.288838709677419 [155, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 156.46987, 0.289166666666667 [156, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 156.80987, 0.289490445859873 [157, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 157.16987, 0.289936708860759 [158, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 157.51987, 0.290314465408805 [159, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 158.59987, 0.29525 [160, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 159.72987, 0.300434782608696 [161, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 21 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.300434782608696 [based on 161 runs with cutoff 5000.0])

   LM for iteration 7: c=0.9 g=20 m=0.1 po=30 pr=1 (0.300434782608696 [based on 161 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 7): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 7): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.300434782608696, based on 161 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.300434782608696 [based on 161 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.300434782608696 [based on 161 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 159.72987, 0.300434782608696 [161, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
same state as last ILS: c=0.9 g=20 m=0.1 po=30 pr=1 (0.300434782608696 [based on 161 runs with cutoff 5000.0])
520/100000000, 159.72987/300.0
iteration 8, flip 31, evaluation count 520
    perturb to ---> c=0.9 g=40 m=0.1 po=30 pr=1 (pruned2 [based on 3 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=200 m=0.1 po=30 pr=1 (pruned1 [based on 3 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=200 m=0.1 po=2000 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 8, start with c=0.9 g=200 m=0.1 po=2000 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 2000->400, evaluating ...
          -> Take improving step to neighbour c=0.9 g=200 m=0.1 po=400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) with flip 31

          
============= Performing 1 bonus runs of state: c=0.9 g=200 m=0.1 po=400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.9 g=200 m=0.1 po=400 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])

    Changing g: 200->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->800, evaluating ...
526/100000000, 162.66987/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->1200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 200->20, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 3 runs with cutoff 5000.0]) with flip 32

          
============= Performing 12 bonus runs of state: c=0.9 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 3 runs with cutoff 5000.0]) ============ 

          -> After 12 bonus runs: c=0.9 g=20 m=0.1 po=400 pr=1 (pruned2 [based on 15 runs with cutoff 5000.0])

    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->1200, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->3200, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 400->1600, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 400->30, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=30 pr=1 (0.300434782608696 [based on 161 runs with cutoff 5000.0]) with flip 33

          
============= Performing 9 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.300434782608696 [based on 161 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 167.51987, 0.300679012345679 [162, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 167.86987, 0.300981595092025 [163, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 168.21987, 0.301280487804878 [164, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 169.12987, 0.304969696969697 [165, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 169.46987, 0.305180722891566 [166, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 169.82987, 0.305508982035928 [167, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 170.19987, 0.305892857142857 [168, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 170.53987, 0.306094674556213 [169, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 170.88987, 0.306352941176471 [170, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 9 bonus runs: c=0.9 g=20 m=0.1 po=30 pr=1 (0.306352941176471 [based on 170 runs with cutoff 5000.0])

    Changing g: 20->60, evaluating ...
        -> worse: (pruned2 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned17 [based on 18 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned2 [based on 8 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned30 [based on 36 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned2 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned2 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned5 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned9 [based on 12 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned3 [based on 4 runs with cutoff 5000.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned5 [based on 6 runs with cutoff 5000.0])
          
============= Performing 19 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.306352941176471 [based on 170 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 171.87984, 0.306666666666667 [171, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 172.20984, 0.306802325581395 [172, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 172.52984, 0.306878612716763 [173, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
554/100000000, 172.98984/300.0
 Same incumbent, new precision:
New Incumbent: 172.98984, 0.307758620689655 [174, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 173.90984, 0.311257142857143 [175, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 174.80984, 0.314602272727273 [176, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 175.15984, 0.314802259887006 [177, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 175.86984, 0.317022471910112 [178, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 176.51984, 0.318882681564246 [179, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 177.47984, 0.322444444444444 [180, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 177.82984, 0.322596685082873 [181, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 178.76984, 0.325989010989011 [182, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 179.51984, 0.328306010928962 [183, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 179.80984, 0.328097826086957 [184, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 180.11984, 0.328 [185, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 180.41984, 0.327849462365591 [186, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 180.70984, 0.327647058823529 [187, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 180.99984, 0.327446808510638 [188, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 181.30984, 0.327354497354497 [189, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 19 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.327354497354497 [based on 189 runs with cutoff 5000.0])

   LM for iteration 8: c=0.9 g=20 m=0.1 po=30 pr=1 (0.327354497354497 [based on 189 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 8): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 8): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.327354497354497, based on 189 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.327354497354497 [based on 189 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.327354497354497 [based on 189 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 181.30984, 0.327354497354497 [189, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
same state as last ILS: c=0.9 g=20 m=0.1 po=30 pr=1 (0.327354497354497 [based on 189 runs with cutoff 5000.0])
569/100000000, 181.30984/300.0
iteration 9, flip 35, evaluation count 569
    perturb to ---> c=0.9 g=100 m=0.1 po=30 pr=1 (pruned1 [based on 4 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=100 m=0.1 po=3600 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.9 g=100 m=0.1 po=400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
   BLS in iteration 9, start with c=0.9 g=100 m=0.1 po=400 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->4000, evaluating ...
573/100000000, 183.26984/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->2400, evaluating ...
          -> Take improving step to neighbour c=0.9 g=100 m=0.1 po=2400 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) with flip 35

          
============= Performing 5 bonus runs of state: c=0.9 g=100 m=0.1 po=2400 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 5 bonus runs: c=0.9 g=100 m=0.1 po=2400 pr=1 (pruned1 [based on 7 runs with cutoff 5000.0])

    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 100->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 2400->30, evaluating ...
          -> Take improving step to neighbour c=0.9 g=100 m=0.1 po=30 pr=1 (pruned3 [based on 7 runs with cutoff 5000.0]) with flip 36

          
============= Performing 6 bonus runs of state: c=0.9 g=100 m=0.1 po=30 pr=1 (pruned3 [based on 7 runs with cutoff 5000.0]) ============ 

          -> After 6 bonus runs: c=0.9 g=100 m=0.1 po=30 pr=1 (pruned6 [based on 13 runs with cutoff 5000.0])

    Changing g: 100->40, evaluating ...
          -> Take improving step to neighbour c=0.9 g=40 m=0.1 po=30 pr=1 (pruned12 [based on 13 runs with cutoff 5000.0]) with flip 37

          
============= Performing 1 bonus runs of state: c=0.9 g=40 m=0.1 po=30 pr=1 (pruned12 [based on 13 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.9 g=40 m=0.1 po=30 pr=1 (pruned13 [based on 14 runs with cutoff 5000.0])

    Changing g: 40->60, evaluating ...
        -> worse: (pruned3 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
598/100000000, 193.46968/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned4 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned9 [based on 11 runs with cutoff 5000.0])
    Changing g: 40->20, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=30 pr=1 (0.327354497354497 [based on 189 runs with cutoff 5000.0]) with flip 38

          
============= Performing 16 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.327354497354497 [based on 189 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 202.65964, 0.326894736842105 [190, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 203.02964, 0.327120418848168 [191, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 203.38964, 0.327291666666667 [192, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
623/100000000, 203.75964/300.0
 Same incumbent, new precision:
New Incumbent: 203.75964, 0.327512953367876 [193, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 204.00964, 0.327113402061856 [194, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 204.24964, 0.326666666666667 [195, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 204.48964, 0.326224489795918 [196, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 204.74964, 0.325888324873096 [197, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 204.98964, 0.325454545454545 [198, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 205.21964, 0.324974874371859 [199, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 205.57964, 0.32515 [200, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 205.93964, 0.325323383084577 [201, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 206.33964, 0.325693069306931 [202, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 206.70964, 0.325911330049261 [203, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 207.06964, 0.326078431372549 [204, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 207.43964, 0.326292682926829 [205, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 16 bonus runs: c=0.9 g=20 m=0.1 po=30 pr=1 (0.326292682926829 [based on 205 runs with cutoff 5000.0])

    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned2 [based on 17 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned3 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned28 [based on 37 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned10 [based on 13 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned17 [based on 19 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned6 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned5 [based on 7 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned2 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned2 [based on 9 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
          
============= Performing 20 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.326292682926829 [based on 205 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 207.94963, 0.326504854368932 [206, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 208.32963, 0.326763285024155 [207, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 208.68963, 0.326923076923077 [208, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 209.05963, 0.327129186602871 [209, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 209.30963, 0.326761904761905 [210, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 209.56963, 0.326445497630332 [211, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 209.79963, 0.325990566037736 [212, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 210.15963, 0.326150234741784 [213, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 210.49963, 0.326214953271028 [214, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 210.86963, 0.326418604651163 [215, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 211.29963, 0.326898148148148 [216, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 211.66963, 0.327096774193549 [217, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 212.04963, 0.327339449541285 [218, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 212.26963, 0.326849315068493 [219, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 212.48963, 0.326363636363637 [220, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 212.85963, 0.326561085972851 [221, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 213.21963, 0.326711711711712 [222, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 213.60963, 0.326995515695067 [223, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
655/100000000, 213.97963/300.0
 Same incumbent, new precision:
New Incumbent: 213.97963, 0.3271875 [224, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 214.33963, 0.327333333333334 [225, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 20 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.327333333333334 [based on 225 runs with cutoff 5000.0])

   LM for iteration 9: c=0.9 g=20 m=0.1 po=30 pr=1 (0.327333333333334 [based on 225 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 9): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 9): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.327333333333334, based on 225 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.327333333333334 [based on 225 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.327333333333334 [based on 225 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 214.33963, 0.327333333333334 [225, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
same state as last ILS: c=0.9 g=20 m=0.1 po=30 pr=1 (0.327333333333334 [based on 225 runs with cutoff 5000.0])
656/100000000, 214.33963/300.0
iteration 10, flip 40, evaluation count 656
    perturb to ---> c=0.7 g=20 m=0.1 po=30 pr=1 (pruned10 [based on 13 runs with cutoff 5000.0])
    perturb to ---> c=0.7 g=80 m=0.1 po=30 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.8 g=80 m=0.1 po=30 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
   BLS in iteration 10, start with c=0.8 g=80 m=0.1 po=30 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->20, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.1 po=30 pr=1 (pruned17 [based on 19 runs with cutoff 5000.0]) with flip 40

          
============= Performing 2 bonus runs of state: c=0.8 g=20 m=0.1 po=30 pr=1 (pruned17 [based on 19 runs with cutoff 5000.0]) ============ 

          -> After 2 bonus runs: c=0.8 g=20 m=0.1 po=30 pr=1 (0.267142857142857 [based on 21 runs with cutoff 5000.0])

    Changing m: 0.1->0.3, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=30 pr=1 (0.284108527131783 [based on 129 runs with cutoff 5000.0]) with flip 41

          
============= Performing 1 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.284108527131783 [based on 129 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.8 g=20 m=0.3 po=30 pr=1 (0.283846153846154 [based on 130 runs with cutoff 5000.0])

    Changing g: 20->40, evaluating ...
        -> worse: (pruned3 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned2 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned32 [based on 38 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned2 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned4 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.8->0.6, evaluating ...
        -> worse: (pruned8 [based on 10 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (pruned4 [based on 5 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned2 [based on 5 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned7 [based on 12 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned2 [based on 6 runs with cutoff 5000.0])
          
============= Performing 22 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.283846153846154 [based on 130 runs with cutoff 5000.0]) ============ 

          -> After 22 bonus runs for LM: c=0.8 g=20 m=0.3 po=30 pr=1 (0.277828947368421 [based on 152 runs with cutoff 5000.0])

   LM for iteration 10: c=0.8 g=20 m=0.3 po=30 pr=1 (0.277828947368421 [based on 152 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 10): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 10): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.327333333333334, based on 225 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.277828947368421 [based on 152 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.327333333333334 [based on 225 runs with cutoff 5000.0])
697/100000000, 224.089610000001/300.0
739/100000000, 234.329610000001/300.0
New inc: 0.266355555555556
New Incumbent: 239.599610000001, 0.266355555555556 [225, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
LM better, change incumbent
New Incumbent: 239.599610000001, 0.266355555555556 [225, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
   Accepting new better local optimum: c=0.8 g=20 m=0.3 po=30 pr=1 (0.266355555555556 [based on 225 runs with cutoff 5000.0])
761/100000000, 239.599610000001/300.0
iteration 11, flip 43, evaluation count 761
    perturb to ---> c=0.8 g=20 m=0.3 po=100 pr=1 (pruned2 [based on 5 runs with cutoff 5000.0])
    perturb to ---> c=0.6 g=20 m=0.3 po=100 pr=1 (100000000 [based on 0 runs with cutoff 0])
    perturb to ---> c=0.6 g=60 m=0.3 po=100 pr=1 (100000000 [based on 0 runs with cutoff 0])
   BLS in iteration 11, start with c=0.6 g=60 m=0.3 po=100 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 100->30, evaluating ...
          -> Take improving step to neighbour c=0.6 g=60 m=0.3 po=30 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) with flip 43

          
============= Performing 1 bonus runs of state: c=0.6 g=60 m=0.3 po=30 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=60 m=0.3 po=30 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])

    Changing g: 60->80, evaluating ...
          -> Take improving step to neighbour c=0.6 g=80 m=0.3 po=30 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) with flip 44

          
============= Performing 1 bonus runs of state: c=0.6 g=80 m=0.3 po=30 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.6 g=80 m=0.3 po=30 pr=1 (pruned2 [based on 3 runs with cutoff 5000.0])

    Changing g: 80->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 80->20, evaluating ...
          -> Take improving step to neighbour c=0.6 g=20 m=0.3 po=30 pr=1 (pruned8 [based on 10 runs with cutoff 5000.0]) with flip 45

          
============= Performing 7 bonus runs of state: c=0.6 g=20 m=0.3 po=30 pr=1 (pruned8 [based on 10 runs with cutoff 5000.0]) ============ 

773/100000000, 244.379600000001/300.0
          -> After 7 bonus runs: c=0.6 g=20 m=0.3 po=30 pr=1 (0.346470588235294 [based on 17 runs with cutoff 5000.0])

    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.6->0.9, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.3 po=30 pr=1 (pruned32 [based on 38 runs with cutoff 5000.0]) with flip 46

          
============= Performing 5 bonus runs of state: c=0.9 g=20 m=0.3 po=30 pr=1 (pruned32 [based on 38 runs with cutoff 5000.0]) ============ 

          -> After 5 bonus runs: c=0.9 g=20 m=0.3 po=30 pr=1 (0.423720930232558 [based on 43 runs with cutoff 5000.0])

    Changing po: 30->1600, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned7 [based on 8 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned1 [based on 5 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
800/100000000, 254.739590000001/300.0
        -> worse: (pruned10 [based on 11 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 4 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=30 pr=1 (0.266355555555556 [based on 225 runs with cutoff 5000.0]) with flip 47

          
============= Performing 14 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.266355555555556 [based on 225 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 256.809580000001, 0.266238938053097 [226, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 257.049580000001, 0.266123348017621 [227, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 257.749580000001, 0.268026315789474 [228, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 258.479580000001, 0.270043668122271 [229, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 259.079580000001, 0.271478260869565 [230, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 259.479580000001, 0.272034632034632 [231, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 259.719580000001, 0.271896551724138 [232, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 260.099580000001, 0.272360515021459 [233, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 260.459580000001, 0.272735042735043 [234, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 260.819580000001, 0.273106382978723 [235, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 261.189580000001, 0.273516949152542 [236, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 261.429580000001, 0.27337552742616 [237, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 261.679580000001, 0.27327731092437 [238, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 261.919580000001, 0.273138075313808 [239, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
          -> After 14 bonus runs: c=0.8 g=20 m=0.3 po=30 pr=1 (0.273138075313808 [based on 239 runs with cutoff 5000.0])

    Changing g: 20->60, evaluating ...
        -> worse: (pruned3 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned2 [based on 7 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned2 [based on 6 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned5 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned8 [based on 13 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned3 [based on 6 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned1 [based on 6 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned2 [based on 6 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (pruned22 [based on 23 runs with cutoff 5000.0])
          
============= Performing 20 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.273138075313808 [based on 239 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 262.739570000001, 0.273 [240, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 262.979570000001, 0.272863070539419 [241, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 263.209570000001, 0.272685950413223 [242, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 263.449570000001, 0.272551440329218 [243, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 263.689570000001, 0.272418032786885 [244, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 263.919570000001, 0.272244897959184 [245, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 264.149570000001, 0.272073170731707 [246, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 264.389570000001, 0.271943319838057 [247, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 264.619570000001, 0.271774193548387 [248, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
832/100000000, 264.849570000001/300.0
 Same incumbent, new precision:
New Incumbent: 264.849570000001, 0.271606425702811 [249, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 265.079570000001, 0.27144 [250, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 265.309570000001, 0.271274900398406 [251, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 265.539570000001, 0.271111111111111 [252, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 265.779570000001, 0.27098814229249 [253, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 266.019570000001, 0.270866141732283 [254, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 266.259570000001, 0.270745098039216 [255, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 266.499570000001, 0.270625 [256, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 266.739570000001, 0.270505836575875 [257, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 266.979570000001, 0.270387596899225 [258, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 267.219570000001, 0.27027027027027 [259, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
          -> After 20 bonus runs for LM: c=0.8 g=20 m=0.3 po=30 pr=1 (0.27027027027027 [based on 259 runs with cutoff 5000.0])

   LM for iteration 11: c=0.8 g=20 m=0.3 po=30 pr=1 (0.27027027027027 [based on 259 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 11): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 11): pr=1, m=0.3, c=0.8, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.27027027027027, based on 259 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.27027027027027 [based on 259 runs with cutoff 5000.0])
c=0.8 g=20 m=0.3 po=30 pr=1 (0.27027027027027 [based on 259 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 267.219570000001, 0.27027027027027 [259, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
same state as last ILS: c=0.8 g=20 m=0.3 po=30 pr=1 (0.27027027027027 [based on 259 runs with cutoff 5000.0])
842/100000000, 267.219570000001/300.0
iteration 12, flip 49, evaluation count 842
    perturb to ---> c=0.8 g=100 m=0.3 po=30 pr=1 (pruned2 [based on 6 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=20 m=0.3 po=30 pr=1 (0.27027027027027 [based on 259 runs with cutoff 5000.0])
    perturb to ---> c=0.8 g=20 m=0.3 po=1600 pr=1 (pruned1 [based on 6 runs with cutoff 5000.0])
   BLS in iteration 12, start with c=0.8 g=20 m=0.3 po=1600 pr=1 (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing po: 1600->3200, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=3200 pr=1 (pruned1 [based on 7 runs with cutoff 5000.0]) with flip 49

          
============= Performing 1 bonus runs of state: c=0.8 g=20 m=0.3 po=3200 pr=1 (pruned1 [based on 7 runs with cutoff 5000.0]) ============ 

          -> After 1 bonus runs: c=0.8 g=20 m=0.3 po=3200 pr=1 (pruned1 [based on 8 runs with cutoff 5000.0])

    Changing po: 3200->1200, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->2400, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing po: 3200->2800, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing po: 3200->800, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 3200->3600, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing po: 3200->2000, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 3200->400, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=400 pr=1 (pruned2 [based on 8 runs with cutoff 5000.0]) with flip 50

          
============= Performing 13 bonus runs of state: c=0.8 g=20 m=0.3 po=400 pr=1 (pruned2 [based on 8 runs with cutoff 5000.0]) ============ 

          -> After 13 bonus runs: c=0.8 g=20 m=0.3 po=400 pr=1 (pruned3 [based on 21 runs with cutoff 5000.0])

    Changing po: 400->4000, evaluating ...
        -> worse: (pruned1 [based on 8 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->200, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=200 pr=1 (pruned6 [based on 21 runs with cutoff 5000.0]) with flip 51

          
============= Performing 4 bonus runs of state: c=0.8 g=20 m=0.3 po=200 pr=1 (pruned6 [based on 21 runs with cutoff 5000.0]) ============ 

          -> After 4 bonus runs: c=0.8 g=20 m=0.3 po=200 pr=1 (pruned7 [based on 25 runs with cutoff 5000.0])

    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
861/100000000, 274.919550000001/300.0
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 5000.0])
    Changing po: 200->100, evaluating ...
        -> worse: (pruned3 [based on 7 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 200->30, evaluating ...
          -> Take improving step to neighbour c=0.8 g=20 m=0.3 po=30 pr=1 (0.27027027027027 [based on 259 runs with cutoff 5000.0]) with flip 52

          
============= Performing 10 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.27027027027027 [based on 259 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 275.619550000001, 0.270115384615385 [260, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 275.859550000001, 0.27 [261, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 276.099550000001, 0.269885496183206 [262, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 276.339550000001, 0.269771863117871 [263, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 276.579550000001, 0.269659090909091 [264, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 276.819550000001, 0.269547169811321 [265, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 277.049550000001, 0.269398496240601 [266, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 277.289550000001, 0.269288389513108 [267, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 277.519550000002, 0.269141791044776 [268, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 277.749550000002, 0.268996282527881 [269, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
          -> After 10 bonus runs: c=0.8 g=20 m=0.3 po=30 pr=1 (0.268996282527881 [based on 269 runs with cutoff 5000.0])

    Changing c: 0.8->0.6, evaluating ...
        -> worse: (pruned17 [based on 20 runs with cutoff 5000.0])
    Changing c: 0.8->0.9, evaluating ...
        -> worse: (pruned39 [based on 45 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned3 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned2 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 7 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned9 [based on 14 runs with cutoff 5000.0])
    Changing c: 0.8->0.7, evaluating ...
        -> worse: (pruned9 [based on 12 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned2 [based on 8 runs with cutoff 5000.0])
    Changing m: 0.3->0.2, evaluating ...
        -> worse: (pruned6 [based on 7 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned4 [based on 7 runs with cutoff 5000.0])
    Changing m: 0.3->0.1, evaluating ...
902/100000000, 284.979550000002/300.0
944/100000000, 295.119550000002/300.0
        -> worse: (0.249557522123894 [based on 113 runs with cutoff 5000.0])
          
============= Performing 11 bonus runs of state: c=0.8 g=20 m=0.3 po=30 pr=1 (0.268996282527881 [based on 269 runs with cutoff 5000.0]) ============ 

          -> After 11 bonus runs for LM: c=0.8 g=20 m=0.3 po=30 pr=1 (0.268996282527881 [based on 269 runs with cutoff 5000.0])

   LM for iteration 12: c=0.8 g=20 m=0.3 po=30 pr=1 (0.268996282527881 [based on 269 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 12): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 12): pr=1, m=0.3, c=0.8, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.268996282527881, based on 269 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.8 g=20 m=0.3 po=30 pr=1 (0.268996282527881 [based on 269 runs with cutoff 5000.0])
c=0.8 g=20 m=0.3 po=30 pr=1 (0.268996282527881 [based on 269 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 300.039550000002, 0.268996282527881 [269, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1
same state as last ILS: c=0.8 g=20 m=0.3 po=30 pr=1 (0.268996282527881 [based on 269 runs with cutoff 5000.0])
Final solution for depth 1 with limit N=2000, and cutoff time=5000.0.
New Incumbent: 300.039550000002, 0.268996282527881 [269, 5000.0]. With state c=0.8, g=20, m=0.3, po=30, pr=1

==================================================================
ParamILS is finished.
==================================================================

Final best parameter configuration found: c=0.8, g=20, m=0.3, po=30, pr=1
==================================================================
Active parameters: c=0.8, g=20, m=0.3, po=30, pr=1

==================================================================
Training quality of this final best found parameter configuration: 0.268996282527881, based on 269 runs with cutoff 5000.0
==================================================================


==================================================================
Computing validation result on independent data -- 5 runs with cutoff time 5000.0...
==================================================================
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.24
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.25
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.25
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.25
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.24
Combined result: 0.246

================================================================
Final best parameter configuration: c=0.8, g=20, m=0.3, po=30, pr=1
==================================================================
Active parameters: c=0.8, g=20, m=0.3, po=30, pr=1

================================================================
Training quality of this final best found parameter configuration: 0.268996282527881, based on 269 runs with cutoff 5000.0
Test quality of this final best found parameter configuration: 0.246, based on 5 independent runs with cutoff 5000.0
==================================================================
