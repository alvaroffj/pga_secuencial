Call: /usr/bin/ruby ../scripts/param_ils_2_3_run.rb "-numRun" "0" "-scenariofile" "escenario.txt" "-validN" "5"


seed: 1234
algo: ./pga_secuencial in.txt out.txt
tunerTimeout: 30.0
maxEvals: 100000000
run_obj: runtime
overall_obj: mean
instance_file: in.txt
test_instance_file: in.txt
N: 2000
cutoff_time: 500.0
cutoff_length: 2
R: 10
pertubation_strength_basic: 
pertubation_strength_scaling: false
p_restart: 0.01
Run 1
Level 
========================================================
Starting ILS for level 1, i.e. a limit of N=2000, and cutoff time=500.0.
Current CPU time = 0, this run goes until 30.0 
========================================================
New Incumbent: 0, 100000000 [0, 0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 0.23, 0.23 [1, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
        -> Worse random: c=0.6 g=80 m=0.3 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.9 g=100 m=0.1 po=2400 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.6 g=40 m=0.2 po=2800 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.7 g=80 m=0.3 po=3600 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.9 g=60 m=0.3 po=2800 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.9 g=20 m=0.1 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.9 g=40 m=0.3 po=100 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.9 g=80 m=0.1 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.7 g=60 m=0.1 po=2400 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
        -> Worse random: c=0.6 g=20 m=0.3 po=200 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
   BLS in iteration 1, start with c=0.9 g=20 m=0.1 po=30 pr=1 (0.23 [based on 1 runs with cutoff 500.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->800, evaluating ...
 Same incumbent, new precision:
New Incumbent: 3.89, 0.265 [2, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
          
============= Performing 24 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.265 [based on 2 runs with cutoff 500.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 8.64, 0.32 [3, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 9.06, 0.345 [4, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 9.5, 0.364 [5, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 9.88, 0.366666666666667 [6, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
39/100000000, 10.25/30.0
 Same incumbent, new precision:
New Incumbent: 10.25, 0.367142857142857 [7, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 10.66, 0.3725 [8, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 11.04, 0.373333333333333 [9, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 11.42, 0.374 [10, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 11.79, 0.373636363636364 [11, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 12.16, 0.373333333333333 [12, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 12.55, 0.374615384615385 [13, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 12.93, 0.375 [14, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 13.31, 0.375333333333333 [15, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 13.69, 0.375625 [16, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 14.06, 0.375294117647059 [17, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 14.43, 0.375 [18, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 14.8, 0.374736842105263 [19, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 15.19, 0.3755 [20, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 15.56, 0.375238095238095 [21, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 15.95, 0.375909090909091 [22, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 16.34, 0.376521739130435 [23, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 16.71, 0.37625 [24, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 17.07, 0.3756 [25, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 17.43, 0.375 [26, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 24 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.375 [based on 26 runs with cutoff 500.0])

   LM for iteration 1: c=0.9 g=20 m=0.1 po=30 pr=1 (0.375 [based on 26 runs with cutoff 500.0])

========== DETAILED RESULTS (iteration 1): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 1): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.375, based on 26 runs with cutoff 500.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.375 [based on 26 runs with cutoff 500.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.375 [based on 26 runs with cutoff 500.0])
LM better, change incumbent
New Incumbent: 17.43, 0.375 [26, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
58/100000000, 17.43/30.0
iteration 2, flip 2, evaluation count 58
    perturb to ---> c=0.9 g=20 m=0.1 po=3200 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
    perturb to ---> c=0.9 g=20 m=0.1 po=200 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
    perturb to ---> c=0.9 g=20 m=0.1 po=1600 pr=1 (pruned0 [based on 1 runs with cutoff 500.0])
   BLS in iteration 2, start with c=0.9 g=20 m=0.1 po=1600 pr=1 (pruned1 [based on 2 runs with cutoff 500.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 1600->400, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 2 runs with cutoff 500.0]) with flip 2

          
============= Performing 3 bonus runs of state: c=0.9 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 2 runs with cutoff 500.0]) ============ 

          -> After 3 bonus runs: c=0.9 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 5 runs with cutoff 500.0])

    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 400->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing po: 400->3600, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing po: 400->2000, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing po: 400->3200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 400->2800, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 400->50, evaluating ...
65/100000000, 20.82/30.0
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 500.0])
    Changing po: 400->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing po: 400->1200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing po: 400->30, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=30 pr=1 (0.375 [based on 26 runs with cutoff 500.0]) with flip 3

          
============= Performing 15 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.375 [based on 26 runs with cutoff 500.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 22.22, 0.374444444444444 [27, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 22.58, 0.373928571428571 [28, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 22.94, 0.373448275862069 [29, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 23.3, 0.373 [30, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 23.66, 0.37258064516129 [31, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 24.02, 0.3721875 [32, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 24.38, 0.371818181818182 [33, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 24.75, 0.371764705882353 [34, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 25.11, 0.371428571428571 [35, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 25.47, 0.371111111111111 [36, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 25.84, 0.371081081081081 [37, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 26.22, 0.371315789473684 [38, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 26.59, 0.371282051282051 [39, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 26.97, 0.3715 [40, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 27.33, 0.371219512195122 [41, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 15 bonus runs: c=0.9 g=20 m=0.1 po=30 pr=1 (0.371219512195122 [based on 41 runs with cutoff 500.0])

    Changing po: 30->4000, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing po: 30->800, evaluating ...
        -> worse: (pruned1 [based on 3 runs with cutoff 500.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 500.0])
          
============= Performing 13 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.371219512195122 [based on 41 runs with cutoff 500.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 28.59, 0.371190476190476 [42, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 28.96, 0.371162790697674 [43, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 29.34, 0.371363636363636 [44, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 29.72, 0.371555555555555 [45, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 30.11, 0.37195652173913 [46, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 13 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.37195652173913 [based on 46 runs with cutoff 500.0])

   LM for iteration 2: c=0.9 g=20 m=0.1 po=30 pr=1 (0.37195652173913 [based on 46 runs with cutoff 500.0])

========== DETAILED RESULTS (iteration 2): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 2): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.37195652173913, based on 46 runs with cutoff 500.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.37195652173913 [based on 46 runs with cutoff 500.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.37195652173913 [based on 46 runs with cutoff 500.0])
LM better, change incumbent
New Incumbent: 30.11, 0.37195652173913 [46, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
same state as last ILS: c=0.9 g=20 m=0.1 po=30 pr=1 (0.37195652173913 [based on 46 runs with cutoff 500.0])
Final solution for depth 1 with limit N=2000, and cutoff time=500.0.
New Incumbent: 30.11, 0.37195652173913 [46, 500.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1

==================================================================
ParamILS is finished.
==================================================================

Final best parameter configuration found: c=0.9, g=20, m=0.1, po=30, pr=1
==================================================================
Active parameters: c=0.9, g=20, m=0.1, po=30, pr=1

==================================================================
Training quality of this final best found parameter configuration: 0.37195652173913, based on 46 runs with cutoff 500.0
==================================================================


==================================================================
Computing validation result on independent data -- 5 runs with cutoff time 500.0...
==================================================================
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.37
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.38
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.38
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.38
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.37
Combined result: 0.376

================================================================
Final best parameter configuration: c=0.9, g=20, m=0.1, po=30, pr=1
==================================================================
Active parameters: c=0.9, g=20, m=0.1, po=30, pr=1

================================================================
Training quality of this final best found parameter configuration: 0.37195652173913, based on 46 runs with cutoff 500.0
Test quality of this final best found parameter configuration: 0.376, based on 5 independent runs with cutoff 500.0
==================================================================
