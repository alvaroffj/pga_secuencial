Call: /usr/bin/ruby ../scripts/param_ils_2_3_run.rb "-numRun" "0" "-scenariofile" "escenario.txt" "-validN" "5"


seed: 1234
algo: ./pga_secuencial in.txt out.txt
tunerTimeout: 30.0
maxEvals: 100000000
run_obj: runtime
overall_obj: mean
instance_file: in.txt
test_instance_file: in.txt
N: 2000
cutoff_time: 5000.0
cutoff_length: 2
R: 10
pertubation_strength_basic: 
pertubation_strength_scaling: false
p_restart: 0.01
Run 1
Level 
========================================================
Starting ILS for level 1, i.e. a limit of N=2000, and cutoff time=5000.0.
Current CPU time = 0, this run goes until 30.0 
========================================================
New Incumbent: 0, 100000000 [0, 0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 0.24, 0.24 [1, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
        -> Worse random: c=0.6 g=80 m=0.3 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=100 m=0.1 po=2400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.6 g=40 m=0.2 po=2800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.7 g=80 m=0.3 po=3600 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=60 m=0.3 po=2800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=20 m=0.1 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=40 m=0.3 po=100 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.9 g=80 m=0.1 po=800 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.7 g=60 m=0.1 po=2400 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
        -> Worse random: c=0.6 g=20 m=0.3 po=200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
   BLS in iteration 1, start with c=0.9 g=20 m=0.1 po=30 pr=1 (0.24 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.3, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->4000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->800, evaluating ...
 Same incumbent, new precision:
New Incumbent: 4.13, 0.315 [2, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 30->3600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2400, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2800, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1600, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->3200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing m: 0.1->0.2, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->50, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->2000, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 30->1200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.7, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
          
============= Performing 24 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.315 [based on 2 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 9.27, 0.423333333333333 [3, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 9.84, 0.46 [4, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
37/100000000, 10.4/30.0
 Same incumbent, new precision:
New Incumbent: 10.4, 0.48 [5, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 10.99, 0.498333333333333 [6, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 11.81, 0.544285714285714 [7, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 12.47, 0.55875 [8, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 13.2, 0.577777777777778 [9, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 14.08, 0.608 [10, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 14.65, 0.604545454545454 [11, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 15.21, 0.600833333333333 [12, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 15.77, 0.597692307692308 [13, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 16.36, 0.597142857142857 [14, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 16.94, 0.596 [15, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 17.53, 0.595625 [16, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 18.08, 0.592941176470588 [17, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 18.65, 0.591666666666667 [18, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 19.22, 0.590526315789474 [19, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 19.79, 0.5895 [20, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 20.33, 0.587142857142857 [21, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
54/100000000, 20.91/30.0
 Same incumbent, new precision:
New Incumbent: 20.91, 0.586818181818182 [22, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 21.84, 0.601739130434783 [23, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 22.49, 0.60375 [24, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 23.06, 0.6024 [25, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 23.64, 0.601538461538462 [26, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 24 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.601538461538462 [based on 26 runs with cutoff 5000.0])

   LM for iteration 1: c=0.9 g=20 m=0.1 po=30 pr=1 (0.601538461538462 [based on 26 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 1): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 1): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.601538461538462, based on 26 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.601538461538462 [based on 26 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.601538461538462 [based on 26 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 23.64, 0.601538461538462 [26, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
58/100000000, 23.64/30.0
iteration 2, flip 2, evaluation count 58
    perturb to ---> c=0.9 g=20 m=0.1 po=3200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=20 m=0.1 po=200 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
    perturb to ---> c=0.9 g=20 m=0.1 po=1600 pr=1 (pruned0 [based on 1 runs with cutoff 5000.0])
   BLS in iteration 2, start with c=0.9 g=20 m=0.1 po=1600 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->100, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 1600->400, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) with flip 2

          
============= Performing 3 bonus runs of state: c=0.9 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 2 runs with cutoff 5000.0]) ============ 

          -> After 3 bonus runs: c=0.9 g=20 m=0.1 po=400 pr=1 (pruned1 [based on 5 runs with cutoff 5000.0])

    Changing g: 20->60, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->3600, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->2000, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->3200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing c: 0.9->0.8, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing g: 20->80, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->2800, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->200, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->50, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing g: 20->40, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing c: 0.9->0.6, evaluating ...
        -> worse: (pruned0 [based on 1 runs with cutoff 5000.0])
    Changing po: 400->100, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->1200, evaluating ...
        -> worse: (pruned1 [based on 2 runs with cutoff 5000.0])
    Changing po: 400->30, evaluating ...
          -> Take improving step to neighbour c=0.9 g=20 m=0.1 po=30 pr=1 (0.601538461538462 [based on 26 runs with cutoff 5000.0]) with flip 3

          
============= Performing 15 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.601538461538462 [based on 26 runs with cutoff 5000.0]) ============ 

 Same incumbent, new precision:
New Incumbent: 28.37, 0.595925925925926 [27, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 28.81, 0.590357142857143 [28, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 29.25, 0.585172413793104 [29, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 29.69, 0.580333333333334 [30, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
 Same incumbent, new precision:
New Incumbent: 30.33, 0.582258064516129 [31, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
          -> After 15 bonus runs: c=0.9 g=20 m=0.1 po=30 pr=1 (0.582258064516129 [based on 31 runs with cutoff 5000.0])

          
============= Performing 0 bonus runs of state: c=0.9 g=20 m=0.1 po=30 pr=1 (0.582258064516129 [based on 31 runs with cutoff 5000.0]) ============ 

          -> After 0 bonus runs for LM: c=0.9 g=20 m=0.1 po=30 pr=1 (0.582258064516129 [based on 31 runs with cutoff 5000.0])

   LM for iteration 2: c=0.9 g=20 m=0.1 po=30 pr=1 (0.582258064516129 [based on 31 runs with cutoff 5000.0])

========== DETAILED RESULTS (iteration 2): ==========
================================================

==================================================================
Best parameter configuration found so far (end of iteration 2): pr=1, m=0.1, c=0.9, g=20, po=30
==================================================================
Training quality of this incumbent parameter configuration: 0.582258064516129, based on 31 runs with cutoff 5000.0
==================================================================

Comparing LM against incumbent:
c=0.9 g=20 m=0.1 po=30 pr=1 (0.582258064516129 [based on 31 runs with cutoff 5000.0])
c=0.9 g=20 m=0.1 po=30 pr=1 (0.582258064516129 [based on 31 runs with cutoff 5000.0])
LM better, change incumbent
New Incumbent: 30.33, 0.582258064516129 [31, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1
same state as last ILS: c=0.9 g=20 m=0.1 po=30 pr=1 (0.582258064516129 [based on 31 runs with cutoff 5000.0])
Final solution for depth 1 with limit N=2000, and cutoff time=5000.0.
New Incumbent: 30.33, 0.582258064516129 [31, 5000.0]. With state c=0.9, g=20, m=0.1, po=30, pr=1

==================================================================
ParamILS is finished.
==================================================================

Final best parameter configuration found: c=0.9, g=20, m=0.1, po=30, pr=1
==================================================================
Active parameters: c=0.9, g=20, m=0.1, po=30, pr=1

==================================================================
Training quality of this final best found parameter configuration: 0.582258064516129, based on 31 runs with cutoff 5000.0
==================================================================


==================================================================
Computing validation result on independent data -- 5 runs with cutoff time 5000.0...
==================================================================
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.59
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.49
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.47
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.5
/media/win/Users/Alvaro/Documents/Memoria 2012/pga_secuencial/dist/Debug/Cygwin-Windows/1: 0.48
Combined result: 0.506

================================================================
Final best parameter configuration: c=0.9, g=20, m=0.1, po=30, pr=1
==================================================================
Active parameters: c=0.9, g=20, m=0.1, po=30, pr=1

================================================================
Training quality of this final best found parameter configuration: 0.582258064516129, based on 31 runs with cutoff 5000.0
Test quality of this final best found parameter configuration: 0.506, based on 5 independent runs with cutoff 5000.0
==================================================================
